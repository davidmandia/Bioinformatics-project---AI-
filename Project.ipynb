{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fraction Unbound (Human)\n",
    "Description: Fraction unbound (FU) refers to the proportion of a small molecule drug that is not bound to proteins in the bloodstream of humans. FU is an important pharmacokinetic property because only the unbound fraction of a drug is typically available to exert pharmacological effects or be metabolized and eliminated from the body. Therefore, it directly influences the drug's potency, efficacy, and potential for adverse effects.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In pharmacokinetics and pharmacology, Fraction Unbound (Human), also known as fu (human), refers to the fraction of a drug that is unbound or free in the plasma. It represents the proportion of the drug that is not bound to plasma proteins and is available for distribution and pharmacological action.\n",
    "\n",
    "High Fraction Unbound (fu): A high fraction unbound indicates that a larger portion of the drug is in its free form and available for distribution to tissues and interaction with its target receptors or enzymes. This can lead to increased pharmacological activity and efficacy, as a higher concentration of the drug is present in the bloodstream and able to exert its effects.\n",
    "\n",
    "Low Fraction Unbound (fu): Conversely, a low fraction unbound suggests that a significant portion of the drug is bound to plasma proteins, reducing its availability for distribution and pharmacological action. While a low fu may increase the drug's plasma half-life and stability, it can also decrease its pharmacological activity and efficacy as less free drug is available to interact with target sites.\n",
    "\n",
    "The optimal fraction unbound for a given drug depends on various factors, including its pharmacokinetic and pharmacodynamic properties, therapeutic index, and desired clinical outcomes. Therefore, the significance of the fraction unbound in drug therapy depends on the specific context and the therapeutic goals of the treatment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the information provided, here are some strategies to potentially improve your model:\n",
    "\n",
    "### 1. Feature Engineering:\n",
    "- **Feature Selection**: Analyze the importance of each feature and consider removing irrelevant or redundant ones. You have a large number of features, so feature selection techniques like Recursive Feature Elimination (RFE) or feature importance from a tree-based model could be beneficial.\n",
    "- **Feature Scaling**: Ensure all features are scaled appropriately, especially if they have different scales.\n",
    "\n",
    "### 2. Model Architecture:\n",
    "- **Simplification**: The current model architecture is quite complex, which may lead to overfitting, especially given the large number of features. Consider reducing the number of layers or neurons to simplify the model.\n",
    "- **Regularization**: Regularization techniques like dropout and L2 regularization can help prevent overfitting. However, you should carefully tune the dropout rate and regularization strength to find the optimal balance.\n",
    "\n",
    "### 3. Hyperparameter Tuning:\n",
    "- **Learning Rate**: Experiment with different learning rates to find the one that results in faster convergence without oscillating or diverging.\n",
    "- **Batch Size**: Adjust the batch size and monitor the training dynamics. Smaller batch sizes often generalize better but may result in slower convergence.\n",
    "- **Optimizer**: Besides Adam, try other optimizers such as RMSprop or SGD with momentum, and tune their parameters accordingly.\n",
    "\n",
    "### 4. Training Strategy:\n",
    "- **Early Stopping**: The `EarlyStopping` callback is already included, but you can further tune its parameters like the `patience` to stop training at the right moment.\n",
    "- **Learning Rate Scheduler**: Instead of a fixed learning rate, consider using a learning rate scheduler to dynamically adjust the learning rate during training.\n",
    "\n",
    "### 5. Cross-Validation:\n",
    "- Perform k-fold cross-validation to assess the model's stability and generalization performance across different subsets of the data.\n",
    "\n",
    "### 6. Data Augmentation:\n",
    "- If you have limited data, consider applying data augmentation techniques to artificially increase the size of your training set.\n",
    "\n",
    "### 7. Model Evaluation:\n",
    "- Besides the test loss, evaluate the model on other metrics relevant to your problem, such as accuracy, precision, recall, or F1 score, especially if your problem is a classification task.\n",
    "\n",
    "### Example Workflow:\n",
    "1. **Feature Selection**: Identify the most important features using feature importance techniques.\n",
    "2. **Model Simplification**: Reduce the complexity of the model architecture.\n",
    "3. **Hyperparameter Tuning**: Tune hyperparameters using techniques like grid search or random search.\n",
    "4. **Cross-Validation**: Assess model performance using k-fold cross-validation.\n",
    "5. **Evaluation**: Evaluate the model on multiple metrics and compare with baseline models or other algorithms.\n",
    "\n",
    "By systematically going through these steps and experimenting with different configurations, you should be able to iteratively improve your model's performance. Remember to keep track of your experiments and document your findings to make informed decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rdkit in /usr/local/python/3.10.13/lib/python3.10/site-packages (2023.9.6)\n",
      "Requirement already satisfied: numpy in /home/codespace/.local/lib/python3.10/site-packages (from rdkit) (1.26.4)\n",
      "Requirement already satisfied: Pillow in /home/codespace/.local/lib/python3.10/site-packages (from rdkit) (10.3.0)\n",
      "Collecting Sklearn\n",
      "  Using cached sklearn-0.0.post12.tar.gz (2.6 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[15 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m The 'sklearn' PyPI package is deprecated, use 'scikit-learn'\n",
      "  \u001b[31m   \u001b[0m rather than 'sklearn' for pip commands.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m Here is how to fix this error in the main use cases:\n",
      "  \u001b[31m   \u001b[0m - use 'pip install scikit-learn' rather than 'pip install sklearn'\n",
      "  \u001b[31m   \u001b[0m - replace 'sklearn' by 'scikit-learn' in your pip requirements files\n",
      "  \u001b[31m   \u001b[0m   (requirements.txt, setup.py, setup.cfg, Pipfile, etc ...)\n",
      "  \u001b[31m   \u001b[0m - if the 'sklearn' package is used by one of your dependencies,\n",
      "  \u001b[31m   \u001b[0m   it would be great if you take some time to track which package uses\n",
      "  \u001b[31m   \u001b[0m   'sklearn' instead of 'scikit-learn' and report it to their issue tracker\n",
      "  \u001b[31m   \u001b[0m - as a last resort, set the environment variable\n",
      "  \u001b[31m   \u001b[0m   SKLEARN_ALLOW_DEPRECATED_SKLEARN_PACKAGE_INSTALL=True to avoid this error\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m More information is available at\n",
      "  \u001b[31m   \u001b[0m https://github.com/scikit-learn/sklearn-pypi-package\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[?25h\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
      "\u001b[31m╰─>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
      "\u001b[1;36mhint\u001b[0m: See above for details.\n",
      "Requirement already satisfied: tensorflow in /usr/local/python/3.10.13/lib/python3.10/site-packages (2.16.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from tensorflow) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from tensorflow) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from tensorflow) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in /home/codespace/.local/lib/python3.10/site-packages (from tensorflow) (24.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from tensorflow) (4.25.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/codespace/.local/lib/python3.10/site-packages (from tensorflow) (2.31.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/python/3.10.13/lib/python3.10/site-packages (from tensorflow) (68.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/codespace/.local/lib/python3.10/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/codespace/.local/lib/python3.10/site-packages (from tensorflow) (4.10.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from tensorflow) (1.63.0)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from tensorflow) (2.16.2)\n",
      "Requirement already satisfied: keras>=3.0.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from tensorflow) (3.3.3)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from tensorflow) (0.37.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /home/codespace/.local/lib/python3.10/site-packages (from tensorflow) (1.26.4)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
      "Requirement already satisfied: rich in /usr/local/python/3.10.13/lib/python3.10/site-packages (from keras>=3.0.0->tensorflow) (13.7.1)\n",
      "Requirement already satisfied: namex in /usr/local/python/3.10.13/lib/python3.10/site-packages (from keras>=3.0.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in /usr/local/python/3.10.13/lib/python3.10/site-packages (from keras>=3.0.0->tensorflow) (0.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.2.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/codespace/.local/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow) (2.1.5)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from rich->keras>=3.0.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/codespace/.local/lib/python3.10/site-packages (from rich->keras>=3.0.0->tensorflow) (2.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow) (0.1.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-18 17:27:15.495023: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-05-18 17:27:16.551201: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-05-18 17:27:18.760538: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-18 17:27:22.173537: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "!pip install rdkit\n",
    "!pip install Sklearn\n",
    "!pip install tensorflow\n",
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from keras.models import save_model\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fu = pd.read_csv(\"data/fu_train.csv\", header=0)\n",
    "test = pd.read_csv(\"data/fu_test.csv\", header=0)\n",
    "data_fu= pd.concat([data_fu, test])\n",
    "\n",
    "data_fu.columns = ['smiles', 'label', 'group']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 smiles     label     group  \\\n",
      "0     CC(=N)N1CC[C@H](Oc2ccc(C(Cc3ccc4ccc(C(=N)N)cc4...  0.397940  training   \n",
      "1     N=C(N)c1ccc(CNC(=O)C2CCCN2C(=O)C(NCC(=O)O)C(c2...  0.289037  training   \n",
      "2     Cc1c(CC2=NN(Cc3ccc(F)cc3F)C(=O)CC2)c2cc(F)ccc2...  1.698970  training   \n",
      "3       Cc1ccc2c(c1)c(-c1ccnc3c(Cl)cccc13)c(C)n2CC(=O)O  2.221849  training   \n",
      "4     N=C(N)c1cc2c(OC(COC(=O)Nc3ccccc3CN3CCNCC3)c3cc...  1.301030  training   \n",
      "...                                                 ...       ...       ...   \n",
      "2134                          CCCC(C)(COC(N)=O)COC(N)=O  1.000000      test   \n",
      "2135    COc1cccc(OC)c1C(=O)NC1C(=O)N2C1SC(C)(C)C2C(=O)O  0.430000      test   \n",
      "2136                            Cc1cccc(C)c1NC(=O)C(C)N  0.870000      test   \n",
      "2137                                Cc1cccc(C)c1OCC(C)N  0.392545      test   \n",
      "2138  C[C@H]1C(=NC(=O)C(NOC(C)(C)C(=O)O)c2csc(N)n2)C...  0.376751      test   \n",
      "\n",
      "                                              Molecule  MaxAbsEStateIndex  \\\n",
      "0     <rdkit.Chem.rdchem.Mol object at 0x7817113e80b0>          12.073341   \n",
      "1     <rdkit.Chem.rdchem.Mol object at 0x7817113e8580>          14.068602   \n",
      "2     <rdkit.Chem.rdchem.Mol object at 0x7817113e8660>          14.036069   \n",
      "3     <rdkit.Chem.rdchem.Mol object at 0x7817113e87b0>          11.386073   \n",
      "4     <rdkit.Chem.rdchem.Mol object at 0x7817113e8820>          12.887452   \n",
      "...                                                ...                ...   \n",
      "2134  <rdkit.Chem.rdchem.Mol object at 0x7816d1ab13f0>          10.455919   \n",
      "2135  <rdkit.Chem.rdchem.Mol object at 0x7816d1ab1460>          12.750756   \n",
      "2136  <rdkit.Chem.rdchem.Mol object at 0x7816d1ab14d0>          11.380559   \n",
      "2137  <rdkit.Chem.rdchem.Mol object at 0x7816d1ab1540>           5.615395   \n",
      "2138  <rdkit.Chem.rdchem.Mol object at 0x7816d1ab15b0>          12.554040   \n",
      "\n",
      "      MaxEStateIndex  MinAbsEStateIndex  MinEStateIndex       qed        SPS  \\\n",
      "0          12.073341           0.005830       -0.880421  0.324103  16.515152   \n",
      "1          14.068602           0.021824       -1.070124  0.188924  15.815789   \n",
      "2          14.036069           0.136081       -1.019465  0.621431  14.187500   \n",
      "3          11.386073           0.074240       -0.861337  0.542446  11.346154   \n",
      "4          12.887452           0.022221       -0.532500  0.180119  14.631579   \n",
      "...              ...                ...             ...       ...        ...   \n",
      "2134       10.455919           0.114537       -0.836664  0.692816  10.800000   \n",
      "2135       12.750756           0.195178       -1.046737  0.732855  25.923077   \n",
      "2136       11.380559           0.150556       -0.477772  0.747236  12.285714   \n",
      "2137        5.615395           0.082413        0.082413  0.770174  12.615385   \n",
      "2138       12.554040           0.059352       -4.782487  0.235934  20.142857   \n",
      "\n",
      "      ...  fr_sulfide  fr_sulfonamd  fr_sulfone  fr_term_acetylene  \\\n",
      "0     ...           0             0           0                  0   \n",
      "1     ...           0             0           0                  0   \n",
      "2     ...           0             0           0                  0   \n",
      "3     ...           0             0           0                  0   \n",
      "4     ...           0             0           0                  0   \n",
      "...   ...         ...           ...         ...                ...   \n",
      "2134  ...           0             0           0                  0   \n",
      "2135  ...           1             0           0                  0   \n",
      "2136  ...           0             0           0                  0   \n",
      "2137  ...           0             0           0                  0   \n",
      "2138  ...           0             0           0                  0   \n",
      "\n",
      "      fr_tetrazole  fr_thiazole  fr_thiocyan  fr_thiophene  fr_unbrch_alkane  \\\n",
      "0                0            0            0             0                 0   \n",
      "1                0            0            0             1                 0   \n",
      "2                0            0            0             0                 0   \n",
      "3                0            0            0             0                 0   \n",
      "4                0            0            0             1                 0   \n",
      "...            ...          ...          ...           ...               ...   \n",
      "2134             0            0            0             0                 0   \n",
      "2135             0            0            0             0                 0   \n",
      "2136             0            0            0             0                 0   \n",
      "2137             0            0            0             0                 0   \n",
      "2138             0            1            0             0                 0   \n",
      "\n",
      "      fr_urea  \n",
      "0           0  \n",
      "1           0  \n",
      "2           0  \n",
      "3           0  \n",
      "4           0  \n",
      "...       ...  \n",
      "2134        0  \n",
      "2135        0  \n",
      "2136        0  \n",
      "2137        0  \n",
      "2138        0  \n",
      "\n",
      "[2139 rows x 214 columns]\n"
     ]
    }
   ],
   "source": [
    "# Add a 'Molecule' column with RDKit molecule objects (handling invalid SMILES)\n",
    "data_fu['Molecule'] = data_fu['smiles'].apply(lambda x: Chem.MolFromSmiles(x) if Chem.MolFromSmiles(x) else None)\n",
    "\n",
    "# Function to calculate all descriptors with error handling\n",
    "def calculate_all_descriptors(molecule):\n",
    "    if molecule is None:\n",
    "        # Return NaNs for all descriptors if the molecule is invalid\n",
    "        return {descriptor: np.nan for descriptor, _ in Descriptors.descList}\n",
    "    \n",
    "    descriptors = {}\n",
    "    for descriptor, descriptor_fn in Descriptors.descList:\n",
    "        try:\n",
    "            descriptors[descriptor] = descriptor_fn(molecule)\n",
    "        except Exception as e:\n",
    "            # Handle individual descriptor calculation errors\n",
    "            descriptors[descriptor] = np.nan\n",
    "            print(f\"Error calculating {descriptor}: {e}\")\n",
    "    return descriptors\n",
    "\n",
    "# Apply the descriptor calculation function to the 'Molecule' column\n",
    "descriptor_data = data_fu['Molecule'].apply(calculate_all_descriptors)\n",
    "\n",
    "# Convert the resulting series of dictionaries into a DataFrame\n",
    "descriptor_df = pd.DataFrame(descriptor_data.tolist())\n",
    "\n",
    "# Combine the original DataFrame with the descriptor DataFrame\n",
    "result_df = pd.concat([data_fu.reset_index(drop=True), descriptor_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "# Print the resulting DataFrame\n",
    "#print(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df.columns[result_df.isna().any()].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_desc =  [descr[0] for descr in Descriptors.descList]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = result_df[list_desc].values\n",
    "y = result_df['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# Trying without scaler to capture variability \n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming X and y are your features and target\n",
    "# Ensure X is a DataFrame\n",
    "X = pd.DataFrame(X) if not isinstance(X, pd.DataFrame) else X\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the training data, transform the test data\n",
    "X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)\n",
    "X_test_scaled = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)\n",
    "\n",
    "# Initialize the RandomForestRegressor\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get feature importances\n",
    "importances = rf.feature_importances_\n",
    "\n",
    "# Create a DataFrame for visualization\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': X_train_scaled.columns,\n",
    "    'Importance': importances\n",
    "})\n",
    "\n",
    "# Sort features by importance\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# # Plot feature importances\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.barh(feature_importance_df['Feature'][:20], feature_importance_df['Importance'][:20])\n",
    "# plt.gca().invert_yaxis()\n",
    "# plt.xlabel('Importance')\n",
    "# plt.ylabel('Feature')\n",
    "# plt.title('Top 20 Feature Importances')\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top important features: [123 114 105 103  18  75  24  28  78  99  41   5  97   3  68  19  21  23\n",
      "  95  62  57  67  89  94  59  25  77  79 100  98  27 104  58 102  64 125\n",
      "  22  66  11   4 101  60  54  93  90  71  92  39   2  88  14  87  45  91\n",
      "  96  48  20  17  53   0  76  47  72  61  12 133  40  83  26  16  44  13\n",
      "  81  70  37 118   1  15  50  38  36  85 151  49  84  34  51  80   7 134\n",
      "  35 117  55 201 130 161 169  46  43  65]\n"
     ]
    }
   ],
   "source": [
    "# Select top N important features\n",
    "N = 100\n",
    "top_features = feature_importance_df['Feature'][:N].values\n",
    "print(\"Top important features:\", top_features)\n",
    "\n",
    "# Create a new dataset with top N features, retaining original names\n",
    "X_top = X[top_features]\n",
    "\n",
    "\n",
    "\n",
    "X_train_top, X_test_top, y_train_top, y_test_top = train_test_split(X_top, y, test_size=0.1, random_state=42)\n",
    "\n",
    "X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train_top))\n",
    "X_test_scaled = pd.DataFrame(scaler.transform(X_test_top))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_scaled shape: (1925, 100)\n",
      "y_train shape: (1925,)\n",
      "X_train_scaled top shape: (1925, 100)\n",
      "y_train shape: (1497,)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train_scaled shape:\", X_train_scaled.shape)\n",
    "print(\"y_train shape:\", y_train_top.shape)\n",
    "print(\"X_train_scaled top shape:\", X_train_top.shape)\n",
    "print(\"y_train shape:\", y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.338601 using {'dropout_rate': 0.2, 'learning_rate': 0.01, 'num_neurons': 128, 'regularization_strength': 0.1}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/800\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 5.2838 - val_loss: 2.4260 - learning_rate: 0.0100\n",
      "Epoch 2/800\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.2080 - val_loss: 1.5058 - learning_rate: 0.0100\n",
      "Epoch 3/800\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.4027 - val_loss: 1.1549 - learning_rate: 0.0100\n",
      "Epoch 4/800\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0803 - val_loss: 0.9051 - learning_rate: 0.0100\n",
      "Epoch 5/800\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.8689 - val_loss: 0.8480 - learning_rate: 0.0100\n",
      "Epoch 6/800\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7878 - val_loss: 0.7088 - learning_rate: 0.0100\n",
      "Epoch 7/800\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7135 - val_loss: 0.6453 - learning_rate: 0.0100\n",
      "Epoch 8/800\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6410 - val_loss: 0.5816 - learning_rate: 0.0100\n",
      "Epoch 9/800\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5902 - val_loss: 0.5707 - learning_rate: 0.0100\n",
      "Epoch 10/800\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5635 - val_loss: 0.5407 - learning_rate: 0.0100\n",
      "Epoch 11/800\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5538 - val_loss: 0.4979 - learning_rate: 0.0100\n",
      "Epoch 12/800\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5114 - val_loss: 0.4796 - learning_rate: 0.0100\n",
      "Epoch 13/800\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4918 - val_loss: 0.5616 - learning_rate: 0.0100\n",
      "Epoch 14/800\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5421 - val_loss: 0.5666 - learning_rate: 0.0100\n",
      "Epoch 15/800\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5410 - val_loss: 0.5098 - learning_rate: 0.0100\n",
      "Epoch 16/800\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4639 - val_loss: 0.4441 - learning_rate: 0.0100\n",
      "Epoch 17/800\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4576 - val_loss: 0.5170 - learning_rate: 0.0100\n",
      "Epoch 18/800\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4594 - val_loss: 0.4718 - learning_rate: 0.0100\n",
      "Epoch 19/800\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4534 - val_loss: 0.4354 - learning_rate: 0.0100\n",
      "Epoch 20/800\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4192 - val_loss: 0.4662 - learning_rate: 0.0100\n",
      "Epoch 21/800\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4606 - val_loss: 0.4695 - learning_rate: 0.0100\n",
      "Epoch 22/800\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4285 - val_loss: 0.3918 - learning_rate: 0.0100\n",
      "Epoch 23/800\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4342 - val_loss: 0.4459 - learning_rate: 0.0100\n",
      "Epoch 24/800\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4356 - val_loss: 0.4390 - learning_rate: 0.0100\n",
      "Epoch 25/800\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4068 - val_loss: 0.4063 - learning_rate: 0.0100\n",
      "Epoch 26/800\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4073 - val_loss: 0.3832 - learning_rate: 0.0100\n",
      "Epoch 27/800\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4216 - val_loss: 0.4133 - learning_rate: 0.0100\n",
      "Epoch 28/800\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4069 - val_loss: 0.4091 - learning_rate: 0.0100\n",
      "Epoch 29/800\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4110 - val_loss: 0.3978 - learning_rate: 0.0100\n",
      "Epoch 30/800\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4125 - val_loss: 0.3989 - learning_rate: 0.0100\n",
      "Epoch 31/800\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4307 - val_loss: 0.4098 - learning_rate: 0.0100\n",
      "Epoch 32/800\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4144 - val_loss: 0.3743 - learning_rate: 0.0020\n",
      "Epoch 33/800\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3725 - val_loss: 0.3637 - learning_rate: 0.0020\n",
      "Epoch 34/800\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3648 - val_loss: 0.3599 - learning_rate: 0.0020\n",
      "Epoch 35/800\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3753 - val_loss: 0.3539 - learning_rate: 0.0020\n",
      "Epoch 36/800\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3806 - val_loss: 0.3443 - learning_rate: 0.0020\n",
      "Epoch 37/800\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3549 - val_loss: 0.3438 - learning_rate: 0.0020\n",
      "Epoch 38/800\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3509 - val_loss: 0.3615 - learning_rate: 0.0020\n",
      "Epoch 39/800\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3587 - val_loss: 0.3504 - learning_rate: 0.0020\n",
      "Epoch 40/800\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.3524 - val_loss: 0.3447 - learning_rate: 0.0020\n",
      "Epoch 41/800\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.3659 - val_loss: 0.3431 - learning_rate: 0.0020\n",
      "Epoch 42/800\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.3415 - val_loss: 0.3363 - learning_rate: 0.0020\n",
      "Epoch 43/800\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3630 - val_loss: 0.3415 - learning_rate: 0.0020\n",
      "Epoch 44/800\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3425 - val_loss: 0.3443 - learning_rate: 0.0020\n",
      "Epoch 45/800\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3464 - val_loss: 0.3438 - learning_rate: 0.0020\n",
      "Epoch 46/800\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3358 - val_loss: 0.3435 - learning_rate: 0.0020\n",
      "Epoch 47/800\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3568 - val_loss: 0.3635 - learning_rate: 0.0020\n",
      "Epoch 48/800\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3297 - val_loss: 0.3470 - learning_rate: 4.0000e-04\n",
      "Epoch 49/800\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3276 - val_loss: 0.3376 - learning_rate: 4.0000e-04\n",
      "Epoch 50/800\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.3420 - val_loss: 0.3340 - learning_rate: 4.0000e-04\n",
      "Epoch 51/800\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3257 - val_loss: 0.3322 - learning_rate: 4.0000e-04\n",
      "Epoch 52/800\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3173 - val_loss: 0.3318 - learning_rate: 4.0000e-04\n",
      "Epoch 53/800\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3225 - val_loss: 0.3326 - learning_rate: 4.0000e-04\n",
      "Epoch 54/800\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3393 - val_loss: 0.3313 - learning_rate: 4.0000e-04\n",
      "Epoch 55/800\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3274 - val_loss: 0.3303 - learning_rate: 4.0000e-04\n",
      "Epoch 56/800\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3147 - val_loss: 0.3278 - learning_rate: 4.0000e-04\n",
      "Epoch 57/800\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3157 - val_loss: 0.3275 - learning_rate: 4.0000e-04\n",
      "Epoch 58/800\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3172 - val_loss: 0.3272 - learning_rate: 4.0000e-04\n",
      "Epoch 59/800\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3032 - val_loss: 0.3271 - learning_rate: 4.0000e-04\n",
      "Epoch 60/800\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3259 - val_loss: 0.3273 - learning_rate: 4.0000e-04\n",
      "Epoch 61/800\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3016 - val_loss: 0.3267 - learning_rate: 4.0000e-04\n",
      "Epoch 62/800\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3185 - val_loss: 0.3283 - learning_rate: 4.0000e-04\n",
      "Epoch 63/800\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3231 - val_loss: 0.3267 - learning_rate: 4.0000e-04\n",
      "Epoch 64/800\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3107 - val_loss: 0.3289 - learning_rate: 4.0000e-04\n",
      "Epoch 65/800\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3075 - val_loss: 0.3295 - learning_rate: 4.0000e-04\n",
      "Epoch 66/800\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3136 - val_loss: 0.3266 - learning_rate: 4.0000e-04\n",
      "Epoch 67/800\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3120 - val_loss: 0.3250 - learning_rate: 1.0000e-04\n",
      "Epoch 68/800\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3273 - val_loss: 0.3244 - learning_rate: 1.0000e-04\n",
      "Epoch 69/800\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3134 - val_loss: 0.3247 - learning_rate: 1.0000e-04\n",
      "Epoch 70/800\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3147 - val_loss: 0.3248 - learning_rate: 1.0000e-04\n",
      "Epoch 71/800\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2974 - val_loss: 0.3245 - learning_rate: 1.0000e-04\n",
      "Epoch 72/800\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3109 - val_loss: 0.3251 - learning_rate: 1.0000e-04\n",
      "Epoch 73/800\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3159 - val_loss: 0.3252 - learning_rate: 1.0000e-04\n",
      "Epoch 74/800\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3064 - val_loss: 0.3249 - learning_rate: 1.0000e-04\n",
      "Epoch 75/800\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3309 - val_loss: 0.3250 - learning_rate: 1.0000e-04\n",
      "Epoch 76/800\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2984 - val_loss: 0.3246 - learning_rate: 1.0000e-04\n",
      "Epoch 77/800\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3026 - val_loss: 0.3249 - learning_rate: 1.0000e-04\n",
      "Epoch 78/800\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3255 - val_loss: 0.3240 - learning_rate: 1.0000e-04\n",
      "Epoch 79/800\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3122 - val_loss: 0.3247 - learning_rate: 1.0000e-04\n",
      "Epoch 80/800\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3090 - val_loss: 0.3237 - learning_rate: 1.0000e-04\n",
      "Epoch 81/800\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2998 - val_loss: 0.3235 - learning_rate: 1.0000e-04\n",
      "Epoch 82/800\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3113 - val_loss: 0.3241 - learning_rate: 1.0000e-04\n",
      "Epoch 83/800\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2899 - val_loss: 0.3252 - learning_rate: 1.0000e-04\n",
      "Epoch 84/800\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2978 - val_loss: 0.3258 - learning_rate: 1.0000e-04\n",
      "Epoch 85/800\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2949 - val_loss: 0.3243 - learning_rate: 1.0000e-04\n",
      "Epoch 86/800\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2903 - val_loss: 0.3237 - learning_rate: 1.0000e-04\n",
      "Epoch 87/800\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3154 - val_loss: 0.3231 - learning_rate: 1.0000e-04\n",
      "Epoch 88/800\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3158 - val_loss: 0.3241 - learning_rate: 1.0000e-04\n",
      "Epoch 89/800\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3019 - val_loss: 0.3242 - learning_rate: 1.0000e-04\n",
      "Epoch 90/800\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3008 - val_loss: 0.3242 - learning_rate: 1.0000e-04\n",
      "Epoch 91/800\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3113 - val_loss: 0.3249 - learning_rate: 1.0000e-04\n",
      "Epoch 92/800\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2986 - val_loss: 0.3245 - learning_rate: 1.0000e-04\n",
      "Epoch 93/800\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3041 - val_loss: 0.3244 - learning_rate: 1.0000e-04\n",
      "Epoch 94/800\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3140 - val_loss: 0.3235 - learning_rate: 1.0000e-04\n",
      "Epoch 95/800\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3307 - val_loss: 0.3235 - learning_rate: 1.0000e-04\n",
      "Epoch 96/800\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3051 - val_loss: 0.3235 - learning_rate: 1.0000e-04\n",
      "Epoch 97/800\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3056 - val_loss: 0.3236 - learning_rate: 1.0000e-04\n",
      "Epoch 98/800\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3007 - val_loss: 0.3240 - learning_rate: 1.0000e-04\n",
      "Epoch 99/800\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2921 - val_loss: 0.3235 - learning_rate: 1.0000e-04\n",
      "Epoch 100/800\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3070 - val_loss: 0.3239 - learning_rate: 1.0000e-04\n",
      "Epoch 101/800\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3034 - val_loss: 0.3236 - learning_rate: 1.0000e-04\n",
      "Epoch 102/800\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2972 - val_loss: 0.3244 - learning_rate: 1.0000e-04\n",
      "Epoch 103/800\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2943 - val_loss: 0.3241 - learning_rate: 1.0000e-04\n",
      "Epoch 104/800\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3004 - val_loss: 0.3238 - learning_rate: 1.0000e-04\n",
      "Epoch 105/800\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3124 - val_loss: 0.3235 - learning_rate: 1.0000e-04\n",
      "Epoch 106/800\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3062 - val_loss: 0.3243 - learning_rate: 1.0000e-04\n",
      "Epoch 107/800\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2999 - val_loss: 0.3242 - learning_rate: 1.0000e-04\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from keras import models, layers, callbacks, optimizers\n",
    "\n",
    "# Define the model with improved architecture and regularization\n",
    "model = models.Sequential([\n",
    "    layers.Input(shape=(X_train_scaled.shape[1],)), \n",
    "    layers.Dense(128, activation='relu', kernel_regularizer='l2'),\n",
    "    layers.Dropout(0.2),  \n",
    "    layers.Dense(64, activation='relu', kernel_regularizer='l2'),  \n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(64, activation='relu', kernel_regularizer='l2'),  \n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(64, activation='relu', kernel_regularizer='l2'),  \n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(64, activation='relu', kernel_regularizer='l2'),  \n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(64, activation='relu', kernel_regularizer='l2'),  \n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(32, activation='relu', kernel_regularizer='l2'),  \n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(8, activation='relu', kernel_regularizer='l2'), \n",
    "    layers.Dense(1,)  # Output layer\n",
    "])\n",
    "\n",
    "# Compile the model with a lower learning rate and Adam optimizer\n",
    "learning_rate = 0.01\n",
    "optimizer = optimizers.Adam(learning_rate=learning_rate)\n",
    "model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "\n",
    "# Define callbacks for early stopping and learning rate reduction\n",
    "early_stopping = callbacks.EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "reduce_lr = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.0001)\n",
    "\n",
    "# Train the model with more epochs and callbacks\n",
    "history = model.fit(X_train_scaled, y_train_top, \n",
    "                    epochs=800, \n",
    "                    validation_split=0.3, \n",
    "                    batch_size=50, \n",
    "                    verbose=1,\n",
    "                    callbacks=[early_stopping, reduce_lr])\n",
    "\n",
    "# Plotting training and validation loss\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['loss'], label='train loss')\n",
    "plt.plot(history.history['val_loss'], label='val loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 8.2251 - val_loss: 7.1228 - learning_rate: 0.1000\n",
      "Epoch 2/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.9215 - val_loss: 6.2390 - learning_rate: 0.1000\n",
      "Epoch 3/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.1024 - val_loss: 5.4440 - learning_rate: 0.1000\n",
      "Epoch 4/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.3306 - val_loss: 4.7559 - learning_rate: 0.1000\n",
      "Epoch 5/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.6503 - val_loss: 4.2204 - learning_rate: 0.1000\n",
      "Epoch 6/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.0866 - val_loss: 3.6224 - learning_rate: 0.1000\n",
      "Epoch 7/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.5864 - val_loss: 3.1985 - learning_rate: 0.1000\n",
      "Epoch 8/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.1253 - val_loss: 2.8550 - learning_rate: 0.1000\n",
      "Epoch 9/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.7919 - val_loss: 2.5090 - learning_rate: 0.1000\n",
      "Epoch 10/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.4542 - val_loss: 2.2105 - learning_rate: 0.1000\n",
      "Epoch 11/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.1709 - val_loss: 1.9405 - learning_rate: 0.1000\n",
      "Epoch 12/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.9346 - val_loss: 1.7350 - learning_rate: 0.1000\n",
      "Epoch 13/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7290 - val_loss: 1.5629 - learning_rate: 0.1000\n",
      "Epoch 14/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.5210 - val_loss: 1.3964 - learning_rate: 0.1000\n",
      "Epoch 15/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.3927 - val_loss: 1.3014 - learning_rate: 0.1000\n",
      "Epoch 16/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.2384 - val_loss: 1.1466 - learning_rate: 0.1000\n",
      "Epoch 17/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.1413 - val_loss: 1.0746 - learning_rate: 0.1000\n",
      "Epoch 18/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0277 - val_loss: 0.9375 - learning_rate: 0.1000\n",
      "Epoch 19/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.9477 - val_loss: 0.8648 - learning_rate: 0.1000\n",
      "Epoch 20/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8498 - val_loss: 0.7934 - learning_rate: 0.1000\n",
      "Epoch 21/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7867 - val_loss: 0.7338 - learning_rate: 0.1000\n",
      "Epoch 22/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7253 - val_loss: 0.7140 - learning_rate: 0.1000\n",
      "Epoch 23/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6773 - val_loss: 0.7207 - learning_rate: 0.1000\n",
      "Epoch 24/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6605 - val_loss: 0.5904 - learning_rate: 0.1000\n",
      "Epoch 25/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6066 - val_loss: 0.5953 - learning_rate: 0.1000\n",
      "Epoch 26/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5769 - val_loss: 0.5266 - learning_rate: 0.1000\n",
      "Epoch 27/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5468 - val_loss: 0.4992 - learning_rate: 0.1000\n",
      "Epoch 28/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5110 - val_loss: 0.5627 - learning_rate: 0.1000\n",
      "Epoch 29/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5168 - val_loss: 0.4670 - learning_rate: 0.1000\n",
      "Epoch 30/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4566 - val_loss: 0.5581 - learning_rate: 0.1000\n",
      "Epoch 31/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4892 - val_loss: 0.4433 - learning_rate: 0.1000\n",
      "Epoch 32/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4319 - val_loss: 0.4813 - learning_rate: 0.1000\n",
      "Epoch 33/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4338 - val_loss: 0.4384 - learning_rate: 0.1000\n",
      "Epoch 34/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4028 - val_loss: 0.4122 - learning_rate: 0.1000\n",
      "Epoch 35/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4085 - val_loss: 0.5309 - learning_rate: 0.1000\n",
      "Epoch 36/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4570 - val_loss: 0.4483 - learning_rate: 0.1000\n",
      "Epoch 37/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4067 - val_loss: 0.4197 - learning_rate: 0.1000\n",
      "Epoch 38/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4015 - val_loss: 0.3691 - learning_rate: 0.1000\n",
      "Epoch 39/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3635 - val_loss: 0.3644 - learning_rate: 0.1000\n",
      "Epoch 40/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3601 - val_loss: 0.3648 - learning_rate: 0.1000\n",
      "Epoch 41/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3358 - val_loss: 0.3957 - learning_rate: 0.1000\n",
      "Epoch 42/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3750 - val_loss: 0.3871 - learning_rate: 0.1000\n",
      "Epoch 43/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3930 - val_loss: 0.3867 - learning_rate: 0.1000\n",
      "Epoch 44/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3687 - val_loss: 0.3545 - learning_rate: 0.1000\n",
      "Epoch 45/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3612 - val_loss: 0.3534 - learning_rate: 0.1000\n",
      "Epoch 46/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3633 - val_loss: 0.4504 - learning_rate: 0.1000\n",
      "Epoch 47/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3622 - val_loss: 0.3634 - learning_rate: 0.1000\n",
      "Epoch 48/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3475 - val_loss: 0.3692 - learning_rate: 0.1000\n",
      "Epoch 49/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3649 - val_loss: 0.3561 - learning_rate: 0.1000\n",
      "Epoch 50/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3474 - val_loss: 0.3724 - learning_rate: 0.1000\n",
      "Epoch 51/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3258 - val_loss: 0.3391 - learning_rate: 0.0200\n",
      "Epoch 52/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3255 - val_loss: 0.3420 - learning_rate: 0.0200\n",
      "Epoch 53/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3073 - val_loss: 0.3359 - learning_rate: 0.0200\n",
      "Epoch 54/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3083 - val_loss: 0.3393 - learning_rate: 0.0200\n",
      "Epoch 55/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3002 - val_loss: 0.3328 - learning_rate: 0.0200\n",
      "Epoch 56/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2972 - val_loss: 0.3384 - learning_rate: 0.0200\n",
      "Epoch 57/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2884 - val_loss: 0.3405 - learning_rate: 0.0200\n",
      "Epoch 58/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2930 - val_loss: 0.3324 - learning_rate: 0.0200\n",
      "Epoch 59/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2939 - val_loss: 0.3463 - learning_rate: 0.0200\n",
      "Epoch 60/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2888 - val_loss: 0.3409 - learning_rate: 0.0200\n",
      "Epoch 61/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3010 - val_loss: 0.3355 - learning_rate: 0.0200\n",
      "Epoch 62/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2751 - val_loss: 0.3418 - learning_rate: 0.0200\n",
      "Epoch 63/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2974 - val_loss: 0.3336 - learning_rate: 0.0200\n",
      "Epoch 64/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2821 - val_loss: 0.3318 - learning_rate: 0.0040\n",
      "Epoch 65/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2811 - val_loss: 0.3327 - learning_rate: 0.0040\n",
      "Epoch 66/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2808 - val_loss: 0.3311 - learning_rate: 0.0040\n",
      "Epoch 67/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2689 - val_loss: 0.3313 - learning_rate: 0.0040\n",
      "Epoch 68/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.3004 - val_loss: 0.3320 - learning_rate: 0.0040\n",
      "Epoch 69/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2888 - val_loss: 0.3315 - learning_rate: 0.0040\n",
      "Epoch 70/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2911 - val_loss: 0.3318 - learning_rate: 0.0040\n",
      "Epoch 71/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2799 - val_loss: 0.3314 - learning_rate: 0.0040\n",
      "Epoch 72/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2815 - val_loss: 0.3311 - learning_rate: 8.0000e-04\n",
      "Epoch 73/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2663 - val_loss: 0.3313 - learning_rate: 8.0000e-04\n",
      "Epoch 74/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2750 - val_loss: 0.3313 - learning_rate: 8.0000e-04\n",
      "Epoch 75/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2897 - val_loss: 0.3313 - learning_rate: 8.0000e-04\n",
      "Epoch 76/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2836 - val_loss: 0.3313 - learning_rate: 8.0000e-04\n",
      "Epoch 77/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2741 - val_loss: 0.3311 - learning_rate: 1.6000e-04\n",
      "Epoch 78/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2853 - val_loss: 0.3310 - learning_rate: 1.6000e-04\n",
      "Epoch 79/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2668 - val_loss: 0.3309 - learning_rate: 1.6000e-04\n",
      "Epoch 80/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2770 - val_loss: 0.3308 - learning_rate: 1.6000e-04\n",
      "Epoch 81/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2713 - val_loss: 0.3308 - learning_rate: 1.6000e-04\n",
      "Epoch 82/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2829 - val_loss: 0.3308 - learning_rate: 1.6000e-04\n",
      "Epoch 83/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2783 - val_loss: 0.3308 - learning_rate: 1.6000e-04\n",
      "Epoch 84/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2737 - val_loss: 0.3307 - learning_rate: 1.6000e-04\n",
      "Epoch 85/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2726 - val_loss: 0.3307 - learning_rate: 1.6000e-04\n",
      "Epoch 86/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2602 - val_loss: 0.3307 - learning_rate: 1.6000e-04\n",
      "Epoch 87/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2734 - val_loss: 0.3307 - learning_rate: 1.6000e-04\n",
      "Epoch 88/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2842 - val_loss: 0.3307 - learning_rate: 1.6000e-04\n",
      "Epoch 89/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2665 - val_loss: 0.3307 - learning_rate: 1.6000e-04\n",
      "Epoch 90/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2702 - val_loss: 0.3307 - learning_rate: 1.0000e-04\n",
      "Epoch 91/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2717 - val_loss: 0.3307 - learning_rate: 1.0000e-04\n",
      "Epoch 92/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2782 - val_loss: 0.3307 - learning_rate: 1.0000e-04\n",
      "Epoch 93/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2714 - val_loss: 0.3307 - learning_rate: 1.0000e-04\n",
      "Epoch 94/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2867 - val_loss: 0.3307 - learning_rate: 1.0000e-04\n",
      "Epoch 95/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2730 - val_loss: 0.3307 - learning_rate: 1.0000e-04\n",
      "Epoch 96/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2766 - val_loss: 0.3306 - learning_rate: 1.0000e-04\n",
      "Epoch 97/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2690 - val_loss: 0.3306 - learning_rate: 1.0000e-04\n",
      "Epoch 98/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2772 - val_loss: 0.3305 - learning_rate: 1.0000e-04\n",
      "Epoch 99/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2886 - val_loss: 0.3305 - learning_rate: 1.0000e-04\n",
      "Epoch 100/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2663 - val_loss: 0.3305 - learning_rate: 1.0000e-04\n",
      "Epoch 101/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2805 - val_loss: 0.3305 - learning_rate: 1.0000e-04\n",
      "Epoch 102/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2791 - val_loss: 0.3305 - learning_rate: 1.0000e-04\n",
      "Epoch 103/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2822 - val_loss: 0.3305 - learning_rate: 1.0000e-04\n",
      "Epoch 104/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2707 - val_loss: 0.3304 - learning_rate: 1.0000e-04\n",
      "Epoch 105/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2914 - val_loss: 0.3304 - learning_rate: 1.0000e-04\n",
      "Epoch 106/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2812 - val_loss: 0.3304 - learning_rate: 1.0000e-04\n",
      "Epoch 107/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2683 - val_loss: 0.3304 - learning_rate: 1.0000e-04\n",
      "Epoch 108/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2703 - val_loss: 0.3304 - learning_rate: 1.0000e-04\n",
      "Epoch 109/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2795 - val_loss: 0.3305 - learning_rate: 1.0000e-04\n",
      "Epoch 110/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2706 - val_loss: 0.3305 - learning_rate: 1.0000e-04\n",
      "Epoch 111/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2792 - val_loss: 0.3305 - learning_rate: 1.0000e-04\n",
      "Epoch 112/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2724 - val_loss: 0.3305 - learning_rate: 1.0000e-04\n",
      "Epoch 113/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2767 - val_loss: 0.3305 - learning_rate: 1.0000e-04\n",
      "Epoch 114/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2770 - val_loss: 0.3304 - learning_rate: 1.0000e-04\n",
      "Epoch 115/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2975 - val_loss: 0.3304 - learning_rate: 1.0000e-04\n",
      "Epoch 116/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2798 - val_loss: 0.3303 - learning_rate: 1.0000e-04\n",
      "Epoch 117/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2812 - val_loss: 0.3303 - learning_rate: 1.0000e-04\n",
      "Epoch 118/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2860 - val_loss: 0.3303 - learning_rate: 1.0000e-04\n",
      "Epoch 119/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2641 - val_loss: 0.3303 - learning_rate: 1.0000e-04\n",
      "Epoch 120/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2633 - val_loss: 0.3303 - learning_rate: 1.0000e-04\n",
      "Epoch 121/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2771 - val_loss: 0.3303 - learning_rate: 1.0000e-04\n",
      "Epoch 122/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2685 - val_loss: 0.3303 - learning_rate: 1.0000e-04\n",
      "Epoch 123/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2837 - val_loss: 0.3303 - learning_rate: 1.0000e-04\n",
      "Epoch 124/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2865 - val_loss: 0.3302 - learning_rate: 1.0000e-04\n",
      "Epoch 125/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2684 - val_loss: 0.3302 - learning_rate: 1.0000e-04\n",
      "Epoch 126/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2775 - val_loss: 0.3302 - learning_rate: 1.0000e-04\n",
      "Epoch 127/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2810 - val_loss: 0.3302 - learning_rate: 1.0000e-04\n",
      "Epoch 128/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2735 - val_loss: 0.3302 - learning_rate: 1.0000e-04\n",
      "Epoch 129/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2692 - val_loss: 0.3302 - learning_rate: 1.0000e-04\n",
      "Epoch 130/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2808 - val_loss: 0.3303 - learning_rate: 1.0000e-04\n",
      "Epoch 131/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2797 - val_loss: 0.3302 - learning_rate: 1.0000e-04\n",
      "Epoch 132/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2693 - val_loss: 0.3302 - learning_rate: 1.0000e-04\n",
      "Epoch 133/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2753 - val_loss: 0.3302 - learning_rate: 1.0000e-04\n",
      "Epoch 134/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2644 - val_loss: 0.3302 - learning_rate: 1.0000e-04\n",
      "Epoch 135/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2801 - val_loss: 0.3301 - learning_rate: 1.0000e-04\n",
      "Epoch 136/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2764 - val_loss: 0.3301 - learning_rate: 1.0000e-04\n",
      "Epoch 137/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2760 - val_loss: 0.3301 - learning_rate: 1.0000e-04\n",
      "Epoch 138/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2766 - val_loss: 0.3301 - learning_rate: 1.0000e-04\n",
      "Epoch 139/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2818 - val_loss: 0.3300 - learning_rate: 1.0000e-04\n",
      "Epoch 140/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2765 - val_loss: 0.3300 - learning_rate: 1.0000e-04\n",
      "Epoch 141/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2890 - val_loss: 0.3300 - learning_rate: 1.0000e-04\n",
      "Epoch 142/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2739 - val_loss: 0.3300 - learning_rate: 1.0000e-04\n",
      "Epoch 143/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2888 - val_loss: 0.3300 - learning_rate: 1.0000e-04\n",
      "Epoch 144/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2748 - val_loss: 0.3300 - learning_rate: 1.0000e-04\n",
      "Epoch 145/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2773 - val_loss: 0.3299 - learning_rate: 1.0000e-04\n",
      "Epoch 146/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2857 - val_loss: 0.3300 - learning_rate: 1.0000e-04\n",
      "Epoch 147/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2785 - val_loss: 0.3300 - learning_rate: 1.0000e-04\n",
      "Epoch 148/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2775 - val_loss: 0.3300 - learning_rate: 1.0000e-04\n",
      "Epoch 149/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2717 - val_loss: 0.3300 - learning_rate: 1.0000e-04\n",
      "Epoch 150/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2768 - val_loss: 0.3300 - learning_rate: 1.0000e-04\n",
      "Epoch 151/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2728 - val_loss: 0.3300 - learning_rate: 1.0000e-04\n",
      "Epoch 152/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2845 - val_loss: 0.3300 - learning_rate: 1.0000e-04\n",
      "Epoch 153/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2780 - val_loss: 0.3300 - learning_rate: 1.0000e-04\n",
      "Epoch 154/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2735 - val_loss: 0.3300 - learning_rate: 1.0000e-04\n",
      "Epoch 155/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2864 - val_loss: 0.3300 - learning_rate: 1.0000e-04\n",
      "Epoch 156/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2898 - val_loss: 0.3300 - learning_rate: 1.0000e-04\n",
      "Epoch 157/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2789 - val_loss: 0.3300 - learning_rate: 1.0000e-04\n",
      "Epoch 158/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2941 - val_loss: 0.3300 - learning_rate: 1.0000e-04\n",
      "Epoch 159/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2728 - val_loss: 0.3300 - learning_rate: 1.0000e-04\n",
      "Epoch 160/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2773 - val_loss: 0.3300 - learning_rate: 1.0000e-04\n",
      "Epoch 161/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2649 - val_loss: 0.3300 - learning_rate: 1.0000e-04\n",
      "Epoch 162/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2827 - val_loss: 0.3300 - learning_rate: 1.0000e-04\n",
      "Epoch 163/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2863 - val_loss: 0.3300 - learning_rate: 1.0000e-04\n",
      "Epoch 164/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2833 - val_loss: 0.3300 - learning_rate: 1.0000e-04\n",
      "Epoch 165/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2693 - val_loss: 0.3300 - learning_rate: 1.0000e-04\n",
      "Epoch 166/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2759 - val_loss: 0.3300 - learning_rate: 1.0000e-04\n",
      "Epoch 167/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2708 - val_loss: 0.3300 - learning_rate: 1.0000e-04\n",
      "Epoch 168/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2837 - val_loss: 0.3300 - learning_rate: 1.0000e-04\n",
      "Epoch 169/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2798 - val_loss: 0.3300 - learning_rate: 1.0000e-04\n",
      "Epoch 170/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2726 - val_loss: 0.3300 - learning_rate: 1.0000e-04\n",
      "Epoch 171/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2771 - val_loss: 0.3300 - learning_rate: 1.0000e-04\n",
      "Epoch 172/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2772 - val_loss: 0.3300 - learning_rate: 1.0000e-04\n",
      "Epoch 173/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2786 - val_loss: 0.3300 - learning_rate: 1.0000e-04\n",
      "Epoch 174/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2723 - val_loss: 0.3300 - learning_rate: 1.0000e-04\n",
      "Epoch 175/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2787 - val_loss: 0.3301 - learning_rate: 1.0000e-04\n",
      "Epoch 176/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2757 - val_loss: 0.3301 - learning_rate: 1.0000e-04\n",
      "Epoch 177/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2762 - val_loss: 0.3300 - learning_rate: 1.0000e-04\n",
      "Epoch 178/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2718 - val_loss: 0.3300 - learning_rate: 1.0000e-04\n",
      "Epoch 179/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2772 - val_loss: 0.3301 - learning_rate: 1.0000e-04\n",
      "Epoch 180/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2729 - val_loss: 0.3300 - learning_rate: 1.0000e-04\n",
      "Epoch 181/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2739 - val_loss: 0.3300 - learning_rate: 1.0000e-04\n",
      "Epoch 182/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2757 - val_loss: 0.3300 - learning_rate: 1.0000e-04\n",
      "Epoch 183/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2928 - val_loss: 0.3299 - learning_rate: 1.0000e-04\n",
      "Epoch 184/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2812 - val_loss: 0.3300 - learning_rate: 1.0000e-04\n",
      "Epoch 185/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2802 - val_loss: 0.3300 - learning_rate: 1.0000e-04\n",
      "Epoch 186/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2699 - val_loss: 0.3300 - learning_rate: 1.0000e-04\n",
      "Epoch 187/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2746 - val_loss: 0.3300 - learning_rate: 1.0000e-04\n",
      "Epoch 188/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2777 - val_loss: 0.3300 - learning_rate: 1.0000e-04\n",
      "Epoch 189/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2725 - val_loss: 0.3300 - learning_rate: 1.0000e-04\n",
      "Epoch 190/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2989 - val_loss: 0.3299 - learning_rate: 1.0000e-04\n",
      "Epoch 191/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2641 - val_loss: 0.3300 - learning_rate: 1.0000e-04\n",
      "Epoch 192/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2774 - val_loss: 0.3299 - learning_rate: 1.0000e-04\n",
      "Epoch 193/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2691 - val_loss: 0.3299 - learning_rate: 1.0000e-04\n",
      "Epoch 194/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2723 - val_loss: 0.3300 - learning_rate: 1.0000e-04\n",
      "Epoch 195/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2835 - val_loss: 0.3299 - learning_rate: 1.0000e-04\n",
      "Epoch 196/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2808 - val_loss: 0.3300 - learning_rate: 1.0000e-04\n",
      "Epoch 197/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2967 - val_loss: 0.3299 - learning_rate: 1.0000e-04\n",
      "Epoch 198/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2696 - val_loss: 0.3299 - learning_rate: 1.0000e-04\n",
      "Epoch 199/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2869 - val_loss: 0.3299 - learning_rate: 1.0000e-04\n",
      "Epoch 200/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2679 - val_loss: 0.3300 - learning_rate: 1.0000e-04\n",
      "Epoch 201/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2780 - val_loss: 0.3300 - learning_rate: 1.0000e-04\n",
      "Epoch 202/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2630 - val_loss: 0.3300 - learning_rate: 1.0000e-04\n",
      "Epoch 203/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2864 - val_loss: 0.3300 - learning_rate: 1.0000e-04\n",
      "Epoch 204/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2771 - val_loss: 0.3300 - learning_rate: 1.0000e-04\n",
      "Epoch 205/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2759 - val_loss: 0.3301 - learning_rate: 1.0000e-04\n",
      "Epoch 206/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2713 - val_loss: 0.3301 - learning_rate: 1.0000e-04\n",
      "Epoch 207/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2777 - val_loss: 0.3301 - learning_rate: 1.0000e-04\n",
      "Epoch 208/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2848 - val_loss: 0.3301 - learning_rate: 1.0000e-04\n",
      "Epoch 209/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2947 - val_loss: 0.3300 - learning_rate: 1.0000e-04\n",
      "Epoch 210/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2786 - val_loss: 0.3300 - learning_rate: 1.0000e-04\n",
      "Epoch 211/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2802 - val_loss: 0.3300 - learning_rate: 1.0000e-04\n",
      "Epoch 212/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2775 - val_loss: 0.3300 - learning_rate: 1.0000e-04\n",
      "Epoch 213/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2947 - val_loss: 0.3300 - learning_rate: 1.0000e-04\n",
      "Epoch 214/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2729 - val_loss: 0.3300 - learning_rate: 1.0000e-04\n",
      "Epoch 215/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2781 - val_loss: 0.3300 - learning_rate: 1.0000e-04\n",
      "Epoch 216/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2841 - val_loss: 0.3300 - learning_rate: 1.0000e-04\n",
      "Epoch 217/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2779 - val_loss: 0.3300 - learning_rate: 1.0000e-04\n",
      "Epoch 218/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2851 - val_loss: 0.3299 - learning_rate: 1.0000e-04\n",
      "Epoch 219/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2886 - val_loss: 0.3299 - learning_rate: 1.0000e-04\n",
      "Epoch 220/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2703 - val_loss: 0.3299 - learning_rate: 1.0000e-04\n",
      "Epoch 221/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2841 - val_loss: 0.3299 - learning_rate: 1.0000e-04\n",
      "Epoch 222/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2620 - val_loss: 0.3299 - learning_rate: 1.0000e-04\n",
      "Epoch 223/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2793 - val_loss: 0.3299 - learning_rate: 1.0000e-04\n",
      "Epoch 224/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2864 - val_loss: 0.3299 - learning_rate: 1.0000e-04\n",
      "Epoch 225/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2814 - val_loss: 0.3300 - learning_rate: 1.0000e-04\n",
      "Epoch 226/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2787 - val_loss: 0.3299 - learning_rate: 1.0000e-04\n",
      "Epoch 227/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2737 - val_loss: 0.3299 - learning_rate: 1.0000e-04\n",
      "Epoch 228/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2782 - val_loss: 0.3299 - learning_rate: 1.0000e-04\n",
      "Epoch 229/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2712 - val_loss: 0.3299 - learning_rate: 1.0000e-04\n",
      "Epoch 230/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2811 - val_loss: 0.3300 - learning_rate: 1.0000e-04\n",
      "Epoch 231/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2771 - val_loss: 0.3300 - learning_rate: 1.0000e-04\n",
      "Epoch 232/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2785 - val_loss: 0.3299 - learning_rate: 1.0000e-04\n",
      "Epoch 233/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2977 - val_loss: 0.3299 - learning_rate: 1.0000e-04\n",
      "Epoch 234/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2816 - val_loss: 0.3299 - learning_rate: 1.0000e-04\n",
      "Epoch 235/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2794 - val_loss: 0.3299 - learning_rate: 1.0000e-04\n",
      "Epoch 236/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2656 - val_loss: 0.3299 - learning_rate: 1.0000e-04\n",
      "Epoch 237/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2696 - val_loss: 0.3299 - learning_rate: 1.0000e-04\n",
      "Epoch 238/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2738 - val_loss: 0.3299 - learning_rate: 1.0000e-04\n",
      "Epoch 239/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2700 - val_loss: 0.3299 - learning_rate: 1.0000e-04\n",
      "Epoch 240/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2756 - val_loss: 0.3299 - learning_rate: 1.0000e-04\n",
      "Epoch 241/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2778 - val_loss: 0.3299 - learning_rate: 1.0000e-04\n",
      "Epoch 242/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2877 - val_loss: 0.3299 - learning_rate: 1.0000e-04\n",
      "Epoch 243/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2732 - val_loss: 0.3299 - learning_rate: 1.0000e-04\n",
      "Epoch 244/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2889 - val_loss: 0.3299 - learning_rate: 1.0000e-04\n",
      "Epoch 245/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2870 - val_loss: 0.3299 - learning_rate: 1.0000e-04\n",
      "Epoch 246/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2734 - val_loss: 0.3299 - learning_rate: 1.0000e-04\n",
      "Epoch 247/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2749 - val_loss: 0.3299 - learning_rate: 1.0000e-04\n",
      "Epoch 248/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2753 - val_loss: 0.3299 - learning_rate: 1.0000e-04\n",
      "Epoch 249/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2697 - val_loss: 0.3299 - learning_rate: 1.0000e-04\n",
      "Epoch 250/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2851 - val_loss: 0.3299 - learning_rate: 1.0000e-04\n",
      "Epoch 251/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2740 - val_loss: 0.3299 - learning_rate: 1.0000e-04\n",
      "Epoch 252/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2669 - val_loss: 0.3299 - learning_rate: 1.0000e-04\n",
      "Epoch 253/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2795 - val_loss: 0.3299 - learning_rate: 1.0000e-04\n",
      "Epoch 254/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2655 - val_loss: 0.3299 - learning_rate: 1.0000e-04\n",
      "Epoch 255/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2638 - val_loss: 0.3299 - learning_rate: 1.0000e-04\n",
      "Epoch 256/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2847 - val_loss: 0.3299 - learning_rate: 1.0000e-04\n",
      "Epoch 257/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2750 - val_loss: 0.3299 - learning_rate: 1.0000e-04\n",
      "Epoch 258/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2687 - val_loss: 0.3299 - learning_rate: 1.0000e-04\n",
      "Epoch 259/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2770 - val_loss: 0.3299 - learning_rate: 1.0000e-04\n",
      "Epoch 260/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2926 - val_loss: 0.3299 - learning_rate: 1.0000e-04\n",
      "Epoch 261/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2734 - val_loss: 0.3299 - learning_rate: 1.0000e-04\n",
      "Epoch 262/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2787 - val_loss: 0.3300 - learning_rate: 1.0000e-04\n",
      "Epoch 263/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2709 - val_loss: 0.3300 - learning_rate: 1.0000e-04\n",
      "Epoch 264/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2881 - val_loss: 0.3300 - learning_rate: 1.0000e-04\n",
      "Epoch 265/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2809 - val_loss: 0.3299 - learning_rate: 1.0000e-04\n",
      "Epoch 266/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2840 - val_loss: 0.3300 - learning_rate: 1.0000e-04\n",
      "Epoch 267/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2734 - val_loss: 0.3300 - learning_rate: 1.0000e-04\n",
      "Epoch 268/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2815 - val_loss: 0.3299 - learning_rate: 1.0000e-04\n",
      "Epoch 269/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2640 - val_loss: 0.3299 - learning_rate: 1.0000e-04\n",
      "Epoch 270/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2794 - val_loss: 0.3299 - learning_rate: 1.0000e-04\n",
      "Epoch 271/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2739 - val_loss: 0.3298 - learning_rate: 1.0000e-04\n",
      "Epoch 272/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2730 - val_loss: 0.3299 - learning_rate: 1.0000e-04\n",
      "Epoch 273/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2668 - val_loss: 0.3298 - learning_rate: 1.0000e-04\n",
      "Epoch 274/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2613 - val_loss: 0.3298 - learning_rate: 1.0000e-04\n",
      "Epoch 275/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2716 - val_loss: 0.3298 - learning_rate: 1.0000e-04\n",
      "Epoch 276/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2830 - val_loss: 0.3298 - learning_rate: 1.0000e-04\n",
      "Epoch 277/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2740 - val_loss: 0.3298 - learning_rate: 1.0000e-04\n",
      "Epoch 278/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2660 - val_loss: 0.3298 - learning_rate: 1.0000e-04\n",
      "Epoch 279/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2744 - val_loss: 0.3298 - learning_rate: 1.0000e-04\n",
      "Epoch 280/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2760 - val_loss: 0.3299 - learning_rate: 1.0000e-04\n",
      "Epoch 281/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2624 - val_loss: 0.3298 - learning_rate: 1.0000e-04\n",
      "Epoch 282/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2803 - val_loss: 0.3298 - learning_rate: 1.0000e-04\n",
      "Epoch 283/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2766 - val_loss: 0.3298 - learning_rate: 1.0000e-04\n",
      "Epoch 284/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2685 - val_loss: 0.3298 - learning_rate: 1.0000e-04\n",
      "Epoch 285/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2844 - val_loss: 0.3299 - learning_rate: 1.0000e-04\n",
      "Epoch 286/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2704 - val_loss: 0.3299 - learning_rate: 1.0000e-04\n",
      "Epoch 287/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2796 - val_loss: 0.3299 - learning_rate: 1.0000e-04\n",
      "Epoch 288/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2723 - val_loss: 0.3298 - learning_rate: 1.0000e-04\n",
      "Epoch 289/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2727 - val_loss: 0.3299 - learning_rate: 1.0000e-04\n",
      "Epoch 290/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2742 - val_loss: 0.3299 - learning_rate: 1.0000e-04\n",
      "Epoch 291/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2642 - val_loss: 0.3299 - learning_rate: 1.0000e-04\n",
      "Epoch 292/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2671 - val_loss: 0.3299 - learning_rate: 1.0000e-04\n",
      "Epoch 293/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2758 - val_loss: 0.3300 - learning_rate: 1.0000e-04\n",
      "Epoch 294/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2741 - val_loss: 0.3299 - learning_rate: 1.0000e-04\n",
      "Epoch 295/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2719 - val_loss: 0.3298 - learning_rate: 1.0000e-04\n",
      "Epoch 296/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2697 - val_loss: 0.3299 - learning_rate: 1.0000e-04\n",
      "Epoch 297/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2784 - val_loss: 0.3299 - learning_rate: 1.0000e-04\n",
      "Epoch 298/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2869 - val_loss: 0.3298 - learning_rate: 1.0000e-04\n",
      "Epoch 299/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2858 - val_loss: 0.3298 - learning_rate: 1.0000e-04\n",
      "Epoch 300/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2698 - val_loss: 0.3298 - learning_rate: 1.0000e-04\n",
      "Epoch 301/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2940 - val_loss: 0.3297 - learning_rate: 1.0000e-04\n",
      "Epoch 302/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2798 - val_loss: 0.3297 - learning_rate: 1.0000e-04\n",
      "Epoch 303/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2770 - val_loss: 0.3298 - learning_rate: 1.0000e-04\n",
      "Epoch 304/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2715 - val_loss: 0.3298 - learning_rate: 1.0000e-04\n",
      "Epoch 305/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2738 - val_loss: 0.3298 - learning_rate: 1.0000e-04\n",
      "Epoch 306/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2776 - val_loss: 0.3298 - learning_rate: 1.0000e-04\n",
      "Epoch 307/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2793 - val_loss: 0.3298 - learning_rate: 1.0000e-04\n",
      "Epoch 308/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2723 - val_loss: 0.3298 - learning_rate: 1.0000e-04\n",
      "Epoch 309/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2845 - val_loss: 0.3299 - learning_rate: 1.0000e-04\n",
      "Epoch 310/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2687 - val_loss: 0.3298 - learning_rate: 1.0000e-04\n",
      "Epoch 311/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2824 - val_loss: 0.3298 - learning_rate: 1.0000e-04\n",
      "Epoch 312/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2765 - val_loss: 0.3298 - learning_rate: 1.0000e-04\n",
      "Epoch 313/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2802 - val_loss: 0.3297 - learning_rate: 1.0000e-04\n",
      "Epoch 314/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2882 - val_loss: 0.3297 - learning_rate: 1.0000e-04\n",
      "Epoch 315/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2827 - val_loss: 0.3297 - learning_rate: 1.0000e-04\n",
      "Epoch 316/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2705 - val_loss: 0.3297 - learning_rate: 1.0000e-04\n",
      "Epoch 317/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2764 - val_loss: 0.3297 - learning_rate: 1.0000e-04\n",
      "Epoch 318/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2785 - val_loss: 0.3297 - learning_rate: 1.0000e-04\n",
      "Epoch 319/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2838 - val_loss: 0.3297 - learning_rate: 1.0000e-04\n",
      "Epoch 320/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2822 - val_loss: 0.3297 - learning_rate: 1.0000e-04\n",
      "Epoch 321/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2925 - val_loss: 0.3297 - learning_rate: 1.0000e-04\n",
      "Epoch 322/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2810 - val_loss: 0.3297 - learning_rate: 1.0000e-04\n",
      "Epoch 323/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2825 - val_loss: 0.3297 - learning_rate: 1.0000e-04\n",
      "Epoch 324/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2656 - val_loss: 0.3297 - learning_rate: 1.0000e-04\n",
      "Epoch 325/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2731 - val_loss: 0.3297 - learning_rate: 1.0000e-04\n",
      "Epoch 326/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2785 - val_loss: 0.3297 - learning_rate: 1.0000e-04\n",
      "Epoch 327/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2650 - val_loss: 0.3296 - learning_rate: 1.0000e-04\n",
      "Epoch 328/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2735 - val_loss: 0.3296 - learning_rate: 1.0000e-04\n",
      "Epoch 329/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2857 - val_loss: 0.3296 - learning_rate: 1.0000e-04\n",
      "Epoch 330/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2674 - val_loss: 0.3296 - learning_rate: 1.0000e-04\n",
      "Epoch 331/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2786 - val_loss: 0.3296 - learning_rate: 1.0000e-04\n",
      "Epoch 332/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2694 - val_loss: 0.3296 - learning_rate: 1.0000e-04\n",
      "Epoch 333/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2750 - val_loss: 0.3296 - learning_rate: 1.0000e-04\n",
      "Epoch 334/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2681 - val_loss: 0.3295 - learning_rate: 1.0000e-04\n",
      "Epoch 335/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2768 - val_loss: 0.3296 - learning_rate: 1.0000e-04\n",
      "Epoch 336/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2736 - val_loss: 0.3295 - learning_rate: 1.0000e-04\n",
      "Epoch 337/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2771 - val_loss: 0.3295 - learning_rate: 1.0000e-04\n",
      "Epoch 338/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2829 - val_loss: 0.3295 - learning_rate: 1.0000e-04\n",
      "Epoch 339/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2749 - val_loss: 0.3295 - learning_rate: 1.0000e-04\n",
      "Epoch 340/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2766 - val_loss: 0.3296 - learning_rate: 1.0000e-04\n",
      "Epoch 341/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2715 - val_loss: 0.3296 - learning_rate: 1.0000e-04\n",
      "Epoch 342/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2909 - val_loss: 0.3296 - learning_rate: 1.0000e-04\n",
      "Epoch 343/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2901 - val_loss: 0.3296 - learning_rate: 1.0000e-04\n",
      "Epoch 344/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2857 - val_loss: 0.3295 - learning_rate: 1.0000e-04\n",
      "Epoch 345/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2822 - val_loss: 0.3295 - learning_rate: 1.0000e-04\n",
      "Epoch 346/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2667 - val_loss: 0.3295 - learning_rate: 1.0000e-04\n",
      "Epoch 347/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2725 - val_loss: 0.3296 - learning_rate: 1.0000e-04\n",
      "Epoch 348/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2819 - val_loss: 0.3296 - learning_rate: 1.0000e-04\n",
      "Epoch 349/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2778 - val_loss: 0.3296 - learning_rate: 1.0000e-04\n",
      "Epoch 350/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2697 - val_loss: 0.3296 - learning_rate: 1.0000e-04\n",
      "Epoch 351/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2729 - val_loss: 0.3296 - learning_rate: 1.0000e-04\n",
      "Epoch 352/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2743 - val_loss: 0.3295 - learning_rate: 1.0000e-04\n",
      "Epoch 353/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2759 - val_loss: 0.3295 - learning_rate: 1.0000e-04\n",
      "Epoch 354/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2855 - val_loss: 0.3295 - learning_rate: 1.0000e-04\n",
      "Epoch 355/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2723 - val_loss: 0.3294 - learning_rate: 1.0000e-04\n",
      "Epoch 356/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2744 - val_loss: 0.3294 - learning_rate: 1.0000e-04\n",
      "Epoch 357/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2850 - val_loss: 0.3294 - learning_rate: 1.0000e-04\n",
      "Epoch 358/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2783 - val_loss: 0.3293 - learning_rate: 1.0000e-04\n",
      "Epoch 359/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2875 - val_loss: 0.3294 - learning_rate: 1.0000e-04\n",
      "Epoch 360/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2845 - val_loss: 0.3292 - learning_rate: 1.0000e-04\n",
      "Epoch 361/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2753 - val_loss: 0.3292 - learning_rate: 1.0000e-04\n",
      "Epoch 362/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2769 - val_loss: 0.3292 - learning_rate: 1.0000e-04\n",
      "Epoch 363/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2770 - val_loss: 0.3292 - learning_rate: 1.0000e-04\n",
      "Epoch 364/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2693 - val_loss: 0.3292 - learning_rate: 1.0000e-04\n",
      "Epoch 365/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2660 - val_loss: 0.3292 - learning_rate: 1.0000e-04\n",
      "Epoch 366/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2763 - val_loss: 0.3292 - learning_rate: 1.0000e-04\n",
      "Epoch 367/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2760 - val_loss: 0.3292 - learning_rate: 1.0000e-04\n",
      "Epoch 368/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2692 - val_loss: 0.3292 - learning_rate: 1.0000e-04\n",
      "Epoch 369/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2767 - val_loss: 0.3293 - learning_rate: 1.0000e-04\n",
      "Epoch 370/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2735 - val_loss: 0.3292 - learning_rate: 1.0000e-04\n",
      "Epoch 371/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2800 - val_loss: 0.3293 - learning_rate: 1.0000e-04\n",
      "Epoch 372/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2683 - val_loss: 0.3292 - learning_rate: 1.0000e-04\n",
      "Epoch 373/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2869 - val_loss: 0.3293 - learning_rate: 1.0000e-04\n",
      "Epoch 374/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2748 - val_loss: 0.3293 - learning_rate: 1.0000e-04\n",
      "Epoch 375/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2783 - val_loss: 0.3293 - learning_rate: 1.0000e-04\n",
      "Epoch 376/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2776 - val_loss: 0.3293 - learning_rate: 1.0000e-04\n",
      "Epoch 377/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2832 - val_loss: 0.3292 - learning_rate: 1.0000e-04\n",
      "Epoch 378/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2848 - val_loss: 0.3292 - learning_rate: 1.0000e-04\n",
      "Epoch 379/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2731 - val_loss: 0.3292 - learning_rate: 1.0000e-04\n",
      "Epoch 380/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2811 - val_loss: 0.3293 - learning_rate: 1.0000e-04\n",
      "Epoch 381/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2739 - val_loss: 0.3292 - learning_rate: 1.0000e-04\n",
      "Epoch 382/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2681 - val_loss: 0.3292 - learning_rate: 1.0000e-04\n",
      "Epoch 383/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2864 - val_loss: 0.3292 - learning_rate: 1.0000e-04\n",
      "Epoch 384/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2817 - val_loss: 0.3292 - learning_rate: 1.0000e-04\n",
      "Epoch 385/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2680 - val_loss: 0.3293 - learning_rate: 1.0000e-04\n",
      "Epoch 386/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2845 - val_loss: 0.3293 - learning_rate: 1.0000e-04\n",
      "Epoch 387/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2728 - val_loss: 0.3292 - learning_rate: 1.0000e-04\n",
      "Epoch 388/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2688 - val_loss: 0.3292 - learning_rate: 1.0000e-04\n",
      "Epoch 389/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2804 - val_loss: 0.3293 - learning_rate: 1.0000e-04\n",
      "Epoch 390/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2931 - val_loss: 0.3293 - learning_rate: 1.0000e-04\n",
      "Epoch 391/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2729 - val_loss: 0.3293 - learning_rate: 1.0000e-04\n",
      "Epoch 392/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2848 - val_loss: 0.3293 - learning_rate: 1.0000e-04\n",
      "Epoch 393/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2867 - val_loss: 0.3292 - learning_rate: 1.0000e-04\n",
      "Epoch 394/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2825 - val_loss: 0.3292 - learning_rate: 1.0000e-04\n",
      "Epoch 395/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2768 - val_loss: 0.3293 - learning_rate: 1.0000e-04\n",
      "Epoch 396/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2789 - val_loss: 0.3292 - learning_rate: 1.0000e-04\n",
      "Epoch 397/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2743 - val_loss: 0.3292 - learning_rate: 1.0000e-04\n",
      "Epoch 398/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2738 - val_loss: 0.3292 - learning_rate: 1.0000e-04\n",
      "Epoch 399/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2800 - val_loss: 0.3292 - learning_rate: 1.0000e-04\n",
      "Epoch 400/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2665 - val_loss: 0.3292 - learning_rate: 1.0000e-04\n",
      "Epoch 401/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2787 - val_loss: 0.3292 - learning_rate: 1.0000e-04\n",
      "Epoch 402/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2746 - val_loss: 0.3291 - learning_rate: 1.0000e-04\n",
      "Epoch 403/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2816 - val_loss: 0.3292 - learning_rate: 1.0000e-04\n",
      "Epoch 404/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2639 - val_loss: 0.3292 - learning_rate: 1.0000e-04\n",
      "Epoch 405/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2746 - val_loss: 0.3292 - learning_rate: 1.0000e-04\n",
      "Epoch 406/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2748 - val_loss: 0.3292 - learning_rate: 1.0000e-04\n",
      "Epoch 407/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2752 - val_loss: 0.3292 - learning_rate: 1.0000e-04\n",
      "Epoch 408/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2729 - val_loss: 0.3292 - learning_rate: 1.0000e-04\n",
      "Epoch 409/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2794 - val_loss: 0.3292 - learning_rate: 1.0000e-04\n",
      "Epoch 410/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2677 - val_loss: 0.3292 - learning_rate: 1.0000e-04\n",
      "Epoch 411/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2738 - val_loss: 0.3291 - learning_rate: 1.0000e-04\n",
      "Epoch 412/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2765 - val_loss: 0.3292 - learning_rate: 1.0000e-04\n",
      "Epoch 413/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2721 - val_loss: 0.3292 - learning_rate: 1.0000e-04\n",
      "Epoch 414/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2750 - val_loss: 0.3291 - learning_rate: 1.0000e-04\n",
      "Epoch 415/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2742 - val_loss: 0.3291 - learning_rate: 1.0000e-04\n",
      "Epoch 416/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2620 - val_loss: 0.3291 - learning_rate: 1.0000e-04\n",
      "Epoch 417/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2710 - val_loss: 0.3291 - learning_rate: 1.0000e-04\n",
      "Epoch 418/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2661 - val_loss: 0.3291 - learning_rate: 1.0000e-04\n",
      "Epoch 419/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2871 - val_loss: 0.3291 - learning_rate: 1.0000e-04\n",
      "Epoch 420/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2772 - val_loss: 0.3291 - learning_rate: 1.0000e-04\n",
      "Epoch 421/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2777 - val_loss: 0.3291 - learning_rate: 1.0000e-04\n",
      "Epoch 422/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2709 - val_loss: 0.3291 - learning_rate: 1.0000e-04\n",
      "Epoch 423/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2855 - val_loss: 0.3291 - learning_rate: 1.0000e-04\n",
      "Epoch 424/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2688 - val_loss: 0.3291 - learning_rate: 1.0000e-04\n",
      "Epoch 425/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2841 - val_loss: 0.3291 - learning_rate: 1.0000e-04\n",
      "Epoch 426/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2697 - val_loss: 0.3291 - learning_rate: 1.0000e-04\n",
      "Epoch 427/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2767 - val_loss: 0.3291 - learning_rate: 1.0000e-04\n",
      "Epoch 428/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2750 - val_loss: 0.3291 - learning_rate: 1.0000e-04\n",
      "Epoch 429/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2583 - val_loss: 0.3291 - learning_rate: 1.0000e-04\n",
      "Epoch 430/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2743 - val_loss: 0.3291 - learning_rate: 1.0000e-04\n",
      "Epoch 431/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2766 - val_loss: 0.3291 - learning_rate: 1.0000e-04\n",
      "Epoch 432/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2714 - val_loss: 0.3291 - learning_rate: 1.0000e-04\n",
      "Epoch 433/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2720 - val_loss: 0.3292 - learning_rate: 1.0000e-04\n",
      "Epoch 434/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2705 - val_loss: 0.3292 - learning_rate: 1.0000e-04\n",
      "Epoch 435/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2935 - val_loss: 0.3293 - learning_rate: 1.0000e-04\n",
      "Epoch 436/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2735 - val_loss: 0.3293 - learning_rate: 1.0000e-04\n",
      "Epoch 437/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2653 - val_loss: 0.3293 - learning_rate: 1.0000e-04\n",
      "Epoch 438/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2739 - val_loss: 0.3293 - learning_rate: 1.0000e-04\n",
      "Epoch 439/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2871 - val_loss: 0.3293 - learning_rate: 1.0000e-04\n",
      "Epoch 440/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2874 - val_loss: 0.3292 - learning_rate: 1.0000e-04\n",
      "Epoch 441/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2721 - val_loss: 0.3292 - learning_rate: 1.0000e-04\n",
      "Epoch 442/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2838 - val_loss: 0.3291 - learning_rate: 1.0000e-04\n",
      "Epoch 443/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2779 - val_loss: 0.3291 - learning_rate: 1.0000e-04\n",
      "Epoch 444/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2630 - val_loss: 0.3291 - learning_rate: 1.0000e-04\n",
      "Epoch 445/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2773 - val_loss: 0.3290 - learning_rate: 1.0000e-04\n",
      "Epoch 446/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2770 - val_loss: 0.3290 - learning_rate: 1.0000e-04\n",
      "Epoch 447/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2764 - val_loss: 0.3290 - learning_rate: 1.0000e-04\n",
      "Epoch 448/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2712 - val_loss: 0.3290 - learning_rate: 1.0000e-04\n",
      "Epoch 449/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2849 - val_loss: 0.3290 - learning_rate: 1.0000e-04\n",
      "Epoch 450/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2717 - val_loss: 0.3290 - learning_rate: 1.0000e-04\n",
      "Epoch 451/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2708 - val_loss: 0.3291 - learning_rate: 1.0000e-04\n",
      "Epoch 452/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2650 - val_loss: 0.3291 - learning_rate: 1.0000e-04\n",
      "Epoch 453/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2743 - val_loss: 0.3290 - learning_rate: 1.0000e-04\n",
      "Epoch 454/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2789 - val_loss: 0.3290 - learning_rate: 1.0000e-04\n",
      "Epoch 455/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2787 - val_loss: 0.3290 - learning_rate: 1.0000e-04\n",
      "Epoch 456/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2737 - val_loss: 0.3290 - learning_rate: 1.0000e-04\n",
      "Epoch 457/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2684 - val_loss: 0.3290 - learning_rate: 1.0000e-04\n",
      "Epoch 458/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2734 - val_loss: 0.3290 - learning_rate: 1.0000e-04\n",
      "Epoch 459/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2734 - val_loss: 0.3289 - learning_rate: 1.0000e-04\n",
      "Epoch 460/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2825 - val_loss: 0.3288 - learning_rate: 1.0000e-04\n",
      "Epoch 461/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2768 - val_loss: 0.3288 - learning_rate: 1.0000e-04\n",
      "Epoch 462/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2853 - val_loss: 0.3288 - learning_rate: 1.0000e-04\n",
      "Epoch 463/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2687 - val_loss: 0.3288 - learning_rate: 1.0000e-04\n",
      "Epoch 464/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2758 - val_loss: 0.3288 - learning_rate: 1.0000e-04\n",
      "Epoch 465/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2735 - val_loss: 0.3289 - learning_rate: 1.0000e-04\n",
      "Epoch 466/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2833 - val_loss: 0.3288 - learning_rate: 1.0000e-04\n",
      "Epoch 467/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2800 - val_loss: 0.3288 - learning_rate: 1.0000e-04\n",
      "Epoch 468/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2714 - val_loss: 0.3288 - learning_rate: 1.0000e-04\n",
      "Epoch 469/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2734 - val_loss: 0.3289 - learning_rate: 1.0000e-04\n",
      "Epoch 470/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2668 - val_loss: 0.3288 - learning_rate: 1.0000e-04\n",
      "Epoch 471/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2775 - val_loss: 0.3288 - learning_rate: 1.0000e-04\n",
      "Epoch 472/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2818 - val_loss: 0.3288 - learning_rate: 1.0000e-04\n",
      "Epoch 473/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2685 - val_loss: 0.3288 - learning_rate: 1.0000e-04\n",
      "Epoch 474/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2754 - val_loss: 0.3288 - learning_rate: 1.0000e-04\n",
      "Epoch 475/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2638 - val_loss: 0.3288 - learning_rate: 1.0000e-04\n",
      "Epoch 476/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2719 - val_loss: 0.3288 - learning_rate: 1.0000e-04\n",
      "Epoch 477/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2744 - val_loss: 0.3287 - learning_rate: 1.0000e-04\n",
      "Epoch 478/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2834 - val_loss: 0.3287 - learning_rate: 1.0000e-04\n",
      "Epoch 479/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2781 - val_loss: 0.3287 - learning_rate: 1.0000e-04\n",
      "Epoch 480/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2664 - val_loss: 0.3287 - learning_rate: 1.0000e-04\n",
      "Epoch 481/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2774 - val_loss: 0.3286 - learning_rate: 1.0000e-04\n",
      "Epoch 482/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2876 - val_loss: 0.3287 - learning_rate: 1.0000e-04\n",
      "Epoch 483/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2794 - val_loss: 0.3287 - learning_rate: 1.0000e-04\n",
      "Epoch 484/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2713 - val_loss: 0.3287 - learning_rate: 1.0000e-04\n",
      "Epoch 485/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2730 - val_loss: 0.3287 - learning_rate: 1.0000e-04\n",
      "Epoch 486/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2723 - val_loss: 0.3288 - learning_rate: 1.0000e-04\n",
      "Epoch 487/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2692 - val_loss: 0.3288 - learning_rate: 1.0000e-04\n",
      "Epoch 488/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2720 - val_loss: 0.3287 - learning_rate: 1.0000e-04\n",
      "Epoch 489/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2706 - val_loss: 0.3287 - learning_rate: 1.0000e-04\n",
      "Epoch 490/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2604 - val_loss: 0.3286 - learning_rate: 1.0000e-04\n",
      "Epoch 491/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2748 - val_loss: 0.3287 - learning_rate: 1.0000e-04\n",
      "Epoch 492/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2765 - val_loss: 0.3287 - learning_rate: 1.0000e-04\n",
      "Epoch 493/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2851 - val_loss: 0.3287 - learning_rate: 1.0000e-04\n",
      "Epoch 494/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2705 - val_loss: 0.3287 - learning_rate: 1.0000e-04\n",
      "Epoch 495/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2810 - val_loss: 0.3287 - learning_rate: 1.0000e-04\n",
      "Epoch 496/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2728 - val_loss: 0.3287 - learning_rate: 1.0000e-04\n",
      "Epoch 497/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2689 - val_loss: 0.3287 - learning_rate: 1.0000e-04\n",
      "Epoch 498/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2645 - val_loss: 0.3287 - learning_rate: 1.0000e-04\n",
      "Epoch 499/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2799 - val_loss: 0.3287 - learning_rate: 1.0000e-04\n",
      "Epoch 500/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2665 - val_loss: 0.3287 - learning_rate: 1.0000e-04\n",
      "Epoch 501/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2752 - val_loss: 0.3286 - learning_rate: 1.0000e-04\n",
      "Epoch 502/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2788 - val_loss: 0.3287 - learning_rate: 1.0000e-04\n",
      "Epoch 503/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2724 - val_loss: 0.3286 - learning_rate: 1.0000e-04\n",
      "Epoch 504/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2730 - val_loss: 0.3287 - learning_rate: 1.0000e-04\n",
      "Epoch 505/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2750 - val_loss: 0.3287 - learning_rate: 1.0000e-04\n",
      "Epoch 506/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2842 - val_loss: 0.3286 - learning_rate: 1.0000e-04\n",
      "Epoch 507/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2756 - val_loss: 0.3287 - learning_rate: 1.0000e-04\n",
      "Epoch 508/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2855 - val_loss: 0.3287 - learning_rate: 1.0000e-04\n",
      "Epoch 509/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2773 - val_loss: 0.3287 - learning_rate: 1.0000e-04\n",
      "Epoch 510/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2758 - val_loss: 0.3287 - learning_rate: 1.0000e-04\n",
      "Epoch 511/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2827 - val_loss: 0.3287 - learning_rate: 1.0000e-04\n",
      "Epoch 512/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2876 - val_loss: 0.3287 - learning_rate: 1.0000e-04\n",
      "Epoch 513/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2864 - val_loss: 0.3287 - learning_rate: 1.0000e-04\n",
      "Epoch 514/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2708 - val_loss: 0.3287 - learning_rate: 1.0000e-04\n",
      "Epoch 515/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2755 - val_loss: 0.3288 - learning_rate: 1.0000e-04\n",
      "Epoch 516/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2650 - val_loss: 0.3288 - learning_rate: 1.0000e-04\n",
      "Epoch 517/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2771 - val_loss: 0.3287 - learning_rate: 1.0000e-04\n",
      "Epoch 518/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2677 - val_loss: 0.3287 - learning_rate: 1.0000e-04\n",
      "Epoch 519/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2701 - val_loss: 0.3287 - learning_rate: 1.0000e-04\n",
      "Epoch 520/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2779 - val_loss: 0.3287 - learning_rate: 1.0000e-04\n",
      "Epoch 521/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2730 - val_loss: 0.3286 - learning_rate: 1.0000e-04\n",
      "Epoch 522/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2676 - val_loss: 0.3286 - learning_rate: 1.0000e-04\n",
      "Epoch 523/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2790 - val_loss: 0.3286 - learning_rate: 1.0000e-04\n",
      "Epoch 524/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2739 - val_loss: 0.3286 - learning_rate: 1.0000e-04\n",
      "Epoch 525/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2733 - val_loss: 0.3286 - learning_rate: 1.0000e-04\n",
      "Epoch 526/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2750 - val_loss: 0.3287 - learning_rate: 1.0000e-04\n",
      "Epoch 527/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2894 - val_loss: 0.3286 - learning_rate: 1.0000e-04\n",
      "Epoch 528/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2654 - val_loss: 0.3287 - learning_rate: 1.0000e-04\n",
      "Epoch 529/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2827 - val_loss: 0.3287 - learning_rate: 1.0000e-04\n",
      "Epoch 530/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2790 - val_loss: 0.3287 - learning_rate: 1.0000e-04\n",
      "Epoch 531/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2730 - val_loss: 0.3287 - learning_rate: 1.0000e-04\n",
      "Epoch 532/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2703 - val_loss: 0.3288 - learning_rate: 1.0000e-04\n",
      "Epoch 533/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2602 - val_loss: 0.3287 - learning_rate: 1.0000e-04\n",
      "Epoch 534/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2749 - val_loss: 0.3286 - learning_rate: 1.0000e-04\n",
      "Epoch 535/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2604 - val_loss: 0.3286 - learning_rate: 1.0000e-04\n",
      "Epoch 536/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2789 - val_loss: 0.3286 - learning_rate: 1.0000e-04\n",
      "Epoch 537/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2838 - val_loss: 0.3287 - learning_rate: 1.0000e-04\n",
      "Epoch 538/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2654 - val_loss: 0.3287 - learning_rate: 1.0000e-04\n",
      "Epoch 539/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2695 - val_loss: 0.3286 - learning_rate: 1.0000e-04\n",
      "Epoch 540/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2733 - val_loss: 0.3286 - learning_rate: 1.0000e-04\n",
      "Epoch 541/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2685 - val_loss: 0.3286 - learning_rate: 1.0000e-04\n",
      "Epoch 542/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2712 - val_loss: 0.3287 - learning_rate: 1.0000e-04\n",
      "Epoch 543/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2718 - val_loss: 0.3287 - learning_rate: 1.0000e-04\n",
      "Epoch 544/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2761 - val_loss: 0.3287 - learning_rate: 1.0000e-04\n",
      "Epoch 545/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2721 - val_loss: 0.3286 - learning_rate: 1.0000e-04\n",
      "Epoch 546/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2863 - val_loss: 0.3286 - learning_rate: 1.0000e-04\n",
      "Epoch 547/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2737 - val_loss: 0.3286 - learning_rate: 1.0000e-04\n",
      "Epoch 548/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2722 - val_loss: 0.3286 - learning_rate: 1.0000e-04\n",
      "Epoch 549/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2822 - val_loss: 0.3286 - learning_rate: 1.0000e-04\n",
      "Epoch 550/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2731 - val_loss: 0.3286 - learning_rate: 1.0000e-04\n",
      "Epoch 551/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2825 - val_loss: 0.3286 - learning_rate: 1.0000e-04\n",
      "Epoch 552/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2719 - val_loss: 0.3286 - learning_rate: 1.0000e-04\n",
      "Epoch 553/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2782 - val_loss: 0.3287 - learning_rate: 1.0000e-04\n",
      "Epoch 554/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2707 - val_loss: 0.3286 - learning_rate: 1.0000e-04\n",
      "Epoch 555/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2776 - val_loss: 0.3286 - learning_rate: 1.0000e-04\n",
      "Epoch 556/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2746 - val_loss: 0.3286 - learning_rate: 1.0000e-04\n",
      "Epoch 557/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2743 - val_loss: 0.3287 - learning_rate: 1.0000e-04\n",
      "Epoch 558/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2704 - val_loss: 0.3287 - learning_rate: 1.0000e-04\n",
      "Epoch 559/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2689 - val_loss: 0.3287 - learning_rate: 1.0000e-04\n",
      "Epoch 560/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2645 - val_loss: 0.3287 - learning_rate: 1.0000e-04\n",
      "Epoch 561/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2720 - val_loss: 0.3287 - learning_rate: 1.0000e-04\n",
      "Epoch 562/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2809 - val_loss: 0.3287 - learning_rate: 1.0000e-04\n",
      "Epoch 563/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2774 - val_loss: 0.3286 - learning_rate: 1.0000e-04\n",
      "Epoch 564/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2723 - val_loss: 0.3286 - learning_rate: 1.0000e-04\n",
      "Epoch 565/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2598 - val_loss: 0.3286 - learning_rate: 1.0000e-04\n",
      "Epoch 566/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2835 - val_loss: 0.3286 - learning_rate: 1.0000e-04\n",
      "Epoch 567/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2714 - val_loss: 0.3286 - learning_rate: 1.0000e-04\n",
      "Epoch 568/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2706 - val_loss: 0.3286 - learning_rate: 1.0000e-04\n",
      "Epoch 569/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2829 - val_loss: 0.3286 - learning_rate: 1.0000e-04\n",
      "Epoch 570/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2738 - val_loss: 0.3286 - learning_rate: 1.0000e-04\n",
      "Epoch 571/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2779 - val_loss: 0.3286 - learning_rate: 1.0000e-04\n",
      "Epoch 572/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2715 - val_loss: 0.3286 - learning_rate: 1.0000e-04\n",
      "Epoch 573/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2833 - val_loss: 0.3287 - learning_rate: 1.0000e-04\n",
      "Epoch 574/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2805 - val_loss: 0.3287 - learning_rate: 1.0000e-04\n",
      "Epoch 575/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2689 - val_loss: 0.3286 - learning_rate: 1.0000e-04\n",
      "Epoch 576/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2785 - val_loss: 0.3287 - learning_rate: 1.0000e-04\n",
      "Epoch 577/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2794 - val_loss: 0.3287 - learning_rate: 1.0000e-04\n",
      "Epoch 578/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2694 - val_loss: 0.3286 - learning_rate: 1.0000e-04\n",
      "Epoch 579/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2692 - val_loss: 0.3286 - learning_rate: 1.0000e-04\n",
      "Epoch 580/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2765 - val_loss: 0.3285 - learning_rate: 1.0000e-04\n",
      "Epoch 581/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2813 - val_loss: 0.3286 - learning_rate: 1.0000e-04\n",
      "Epoch 582/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2753 - val_loss: 0.3286 - learning_rate: 1.0000e-04\n",
      "Epoch 583/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2850 - val_loss: 0.3286 - learning_rate: 1.0000e-04\n",
      "Epoch 584/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2715 - val_loss: 0.3286 - learning_rate: 1.0000e-04\n",
      "Epoch 585/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2838 - val_loss: 0.3286 - learning_rate: 1.0000e-04\n",
      "Epoch 586/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2698 - val_loss: 0.3286 - learning_rate: 1.0000e-04\n",
      "Epoch 587/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2726 - val_loss: 0.3286 - learning_rate: 1.0000e-04\n",
      "Epoch 588/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2647 - val_loss: 0.3286 - learning_rate: 1.0000e-04\n",
      "Epoch 589/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2709 - val_loss: 0.3285 - learning_rate: 1.0000e-04\n",
      "Epoch 590/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2739 - val_loss: 0.3285 - learning_rate: 1.0000e-04\n",
      "Epoch 591/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2758 - val_loss: 0.3285 - learning_rate: 1.0000e-04\n",
      "Epoch 592/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2906 - val_loss: 0.3285 - learning_rate: 1.0000e-04\n",
      "Epoch 593/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2875 - val_loss: 0.3286 - learning_rate: 1.0000e-04\n",
      "Epoch 594/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2754 - val_loss: 0.3286 - learning_rate: 1.0000e-04\n",
      "Epoch 595/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2857 - val_loss: 0.3286 - learning_rate: 1.0000e-04\n",
      "Epoch 596/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2841 - val_loss: 0.3286 - learning_rate: 1.0000e-04\n",
      "Epoch 597/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2721 - val_loss: 0.3286 - learning_rate: 1.0000e-04\n",
      "Epoch 598/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2843 - val_loss: 0.3285 - learning_rate: 1.0000e-04\n",
      "Epoch 599/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2751 - val_loss: 0.3285 - learning_rate: 1.0000e-04\n",
      "Epoch 600/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2759 - val_loss: 0.3285 - learning_rate: 1.0000e-04\n",
      "Epoch 601/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2663 - val_loss: 0.3285 - learning_rate: 1.0000e-04\n",
      "Epoch 602/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2864 - val_loss: 0.3284 - learning_rate: 1.0000e-04\n",
      "Epoch 603/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2616 - val_loss: 0.3284 - learning_rate: 1.0000e-04\n",
      "Epoch 604/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2897 - val_loss: 0.3285 - learning_rate: 1.0000e-04\n",
      "Epoch 605/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2746 - val_loss: 0.3285 - learning_rate: 1.0000e-04\n",
      "Epoch 606/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2579 - val_loss: 0.3285 - learning_rate: 1.0000e-04\n",
      "Epoch 607/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2729 - val_loss: 0.3285 - learning_rate: 1.0000e-04\n",
      "Epoch 608/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2675 - val_loss: 0.3285 - learning_rate: 1.0000e-04\n",
      "Epoch 609/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2728 - val_loss: 0.3285 - learning_rate: 1.0000e-04\n",
      "Epoch 610/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2634 - val_loss: 0.3285 - learning_rate: 1.0000e-04\n",
      "Epoch 611/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2662 - val_loss: 0.3284 - learning_rate: 1.0000e-04\n",
      "Epoch 612/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2816 - val_loss: 0.3284 - learning_rate: 1.0000e-04\n",
      "Epoch 613/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2685 - val_loss: 0.3284 - learning_rate: 1.0000e-04\n",
      "Epoch 614/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2780 - val_loss: 0.3284 - learning_rate: 1.0000e-04\n",
      "Epoch 615/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2704 - val_loss: 0.3284 - learning_rate: 1.0000e-04\n",
      "Epoch 616/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2699 - val_loss: 0.3284 - learning_rate: 1.0000e-04\n",
      "Epoch 617/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2811 - val_loss: 0.3284 - learning_rate: 1.0000e-04\n",
      "Epoch 618/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2769 - val_loss: 0.3284 - learning_rate: 1.0000e-04\n",
      "Epoch 619/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2752 - val_loss: 0.3284 - learning_rate: 1.0000e-04\n",
      "Epoch 620/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2683 - val_loss: 0.3284 - learning_rate: 1.0000e-04\n",
      "Epoch 621/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2788 - val_loss: 0.3283 - learning_rate: 1.0000e-04\n",
      "Epoch 622/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2760 - val_loss: 0.3283 - learning_rate: 1.0000e-04\n",
      "Epoch 623/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2704 - val_loss: 0.3283 - learning_rate: 1.0000e-04\n",
      "Epoch 624/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2767 - val_loss: 0.3283 - learning_rate: 1.0000e-04\n",
      "Epoch 625/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2724 - val_loss: 0.3284 - learning_rate: 1.0000e-04\n",
      "Epoch 626/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2756 - val_loss: 0.3284 - learning_rate: 1.0000e-04\n",
      "Epoch 627/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2958 - val_loss: 0.3284 - learning_rate: 1.0000e-04\n",
      "Epoch 628/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2815 - val_loss: 0.3284 - learning_rate: 1.0000e-04\n",
      "Epoch 629/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2791 - val_loss: 0.3284 - learning_rate: 1.0000e-04\n",
      "Epoch 630/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2809 - val_loss: 0.3284 - learning_rate: 1.0000e-04\n",
      "Epoch 631/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2827 - val_loss: 0.3284 - learning_rate: 1.0000e-04\n",
      "Epoch 632/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2659 - val_loss: 0.3284 - learning_rate: 1.0000e-04\n",
      "Epoch 633/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2697 - val_loss: 0.3284 - learning_rate: 1.0000e-04\n",
      "Epoch 634/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2777 - val_loss: 0.3283 - learning_rate: 1.0000e-04\n",
      "Epoch 635/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2765 - val_loss: 0.3284 - learning_rate: 1.0000e-04\n",
      "Epoch 636/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2759 - val_loss: 0.3284 - learning_rate: 1.0000e-04\n",
      "Epoch 637/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2720 - val_loss: 0.3283 - learning_rate: 1.0000e-04\n",
      "Epoch 638/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2694 - val_loss: 0.3283 - learning_rate: 1.0000e-04\n",
      "Epoch 639/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2818 - val_loss: 0.3283 - learning_rate: 1.0000e-04\n",
      "Epoch 640/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2702 - val_loss: 0.3283 - learning_rate: 1.0000e-04\n",
      "Epoch 641/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2768 - val_loss: 0.3283 - learning_rate: 1.0000e-04\n",
      "Epoch 642/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2685 - val_loss: 0.3282 - learning_rate: 1.0000e-04\n",
      "Epoch 643/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2691 - val_loss: 0.3282 - learning_rate: 1.0000e-04\n",
      "Epoch 644/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2692 - val_loss: 0.3282 - learning_rate: 1.0000e-04\n",
      "Epoch 645/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2762 - val_loss: 0.3282 - learning_rate: 1.0000e-04\n",
      "Epoch 646/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2649 - val_loss: 0.3283 - learning_rate: 1.0000e-04\n",
      "Epoch 647/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2662 - val_loss: 0.3283 - learning_rate: 1.0000e-04\n",
      "Epoch 648/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2785 - val_loss: 0.3283 - learning_rate: 1.0000e-04\n",
      "Epoch 649/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2663 - val_loss: 0.3283 - learning_rate: 1.0000e-04\n",
      "Epoch 650/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2740 - val_loss: 0.3283 - learning_rate: 1.0000e-04\n",
      "Epoch 651/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2791 - val_loss: 0.3284 - learning_rate: 1.0000e-04\n",
      "Epoch 652/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2846 - val_loss: 0.3284 - learning_rate: 1.0000e-04\n",
      "Epoch 653/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2708 - val_loss: 0.3284 - learning_rate: 1.0000e-04\n",
      "Epoch 654/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2887 - val_loss: 0.3283 - learning_rate: 1.0000e-04\n",
      "Epoch 655/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2757 - val_loss: 0.3283 - learning_rate: 1.0000e-04\n",
      "Epoch 656/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2884 - val_loss: 0.3283 - learning_rate: 1.0000e-04\n",
      "Epoch 657/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2661 - val_loss: 0.3283 - learning_rate: 1.0000e-04\n",
      "Epoch 658/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2925 - val_loss: 0.3283 - learning_rate: 1.0000e-04\n",
      "Epoch 659/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2684 - val_loss: 0.3284 - learning_rate: 1.0000e-04\n",
      "Epoch 660/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2746 - val_loss: 0.3284 - learning_rate: 1.0000e-04\n",
      "Epoch 661/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2877 - val_loss: 0.3284 - learning_rate: 1.0000e-04\n",
      "Epoch 662/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2570 - val_loss: 0.3284 - learning_rate: 1.0000e-04\n",
      "Epoch 663/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2773 - val_loss: 0.3284 - learning_rate: 1.0000e-04\n",
      "Epoch 664/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2758 - val_loss: 0.3284 - learning_rate: 1.0000e-04\n",
      "Epoch 665/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2619 - val_loss: 0.3284 - learning_rate: 1.0000e-04\n",
      "Epoch 666/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2780 - val_loss: 0.3284 - learning_rate: 1.0000e-04\n",
      "Epoch 667/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2854 - val_loss: 0.3284 - learning_rate: 1.0000e-04\n",
      "Epoch 668/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2781 - val_loss: 0.3284 - learning_rate: 1.0000e-04\n",
      "Epoch 669/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2883 - val_loss: 0.3284 - learning_rate: 1.0000e-04\n",
      "Epoch 670/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2659 - val_loss: 0.3284 - learning_rate: 1.0000e-04\n",
      "Epoch 671/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2735 - val_loss: 0.3283 - learning_rate: 1.0000e-04\n",
      "Epoch 672/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2659 - val_loss: 0.3284 - learning_rate: 1.0000e-04\n",
      "Epoch 673/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2773 - val_loss: 0.3284 - learning_rate: 1.0000e-04\n",
      "Epoch 674/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2621 - val_loss: 0.3283 - learning_rate: 1.0000e-04\n",
      "Epoch 675/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2725 - val_loss: 0.3283 - learning_rate: 1.0000e-04\n",
      "Epoch 676/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2740 - val_loss: 0.3283 - learning_rate: 1.0000e-04\n",
      "Epoch 677/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2675 - val_loss: 0.3283 - learning_rate: 1.0000e-04\n",
      "Epoch 678/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2842 - val_loss: 0.3282 - learning_rate: 1.0000e-04\n",
      "Epoch 679/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2747 - val_loss: 0.3282 - learning_rate: 1.0000e-04\n",
      "Epoch 680/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2803 - val_loss: 0.3281 - learning_rate: 1.0000e-04\n",
      "Epoch 681/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2719 - val_loss: 0.3282 - learning_rate: 1.0000e-04\n",
      "Epoch 682/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2784 - val_loss: 0.3281 - learning_rate: 1.0000e-04\n",
      "Epoch 683/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2750 - val_loss: 0.3281 - learning_rate: 1.0000e-04\n",
      "Epoch 684/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2627 - val_loss: 0.3282 - learning_rate: 1.0000e-04\n",
      "Epoch 685/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2687 - val_loss: 0.3282 - learning_rate: 1.0000e-04\n",
      "Epoch 686/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2743 - val_loss: 0.3283 - learning_rate: 1.0000e-04\n",
      "Epoch 687/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2755 - val_loss: 0.3282 - learning_rate: 1.0000e-04\n",
      "Epoch 688/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2736 - val_loss: 0.3283 - learning_rate: 1.0000e-04\n",
      "Epoch 689/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2757 - val_loss: 0.3283 - learning_rate: 1.0000e-04\n",
      "Epoch 690/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2783 - val_loss: 0.3283 - learning_rate: 1.0000e-04\n",
      "Epoch 691/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2648 - val_loss: 0.3283 - learning_rate: 1.0000e-04\n",
      "Epoch 692/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2664 - val_loss: 0.3283 - learning_rate: 1.0000e-04\n",
      "Epoch 693/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2771 - val_loss: 0.3282 - learning_rate: 1.0000e-04\n",
      "Epoch 694/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2819 - val_loss: 0.3283 - learning_rate: 1.0000e-04\n",
      "Epoch 695/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2767 - val_loss: 0.3283 - learning_rate: 1.0000e-04\n",
      "Epoch 696/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2742 - val_loss: 0.3283 - learning_rate: 1.0000e-04\n",
      "Epoch 697/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2598 - val_loss: 0.3283 - learning_rate: 1.0000e-04\n",
      "Epoch 698/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2709 - val_loss: 0.3283 - learning_rate: 1.0000e-04\n",
      "Epoch 699/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2641 - val_loss: 0.3282 - learning_rate: 1.0000e-04\n",
      "Epoch 700/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2687 - val_loss: 0.3282 - learning_rate: 1.0000e-04\n",
      "Epoch 701/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2691 - val_loss: 0.3282 - learning_rate: 1.0000e-04\n",
      "Epoch 702/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2781 - val_loss: 0.3282 - learning_rate: 1.0000e-04\n",
      "Epoch 703/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2671 - val_loss: 0.3282 - learning_rate: 1.0000e-04\n",
      "Epoch 704/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2821 - val_loss: 0.3282 - learning_rate: 1.0000e-04\n",
      "Epoch 705/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2774 - val_loss: 0.3282 - learning_rate: 1.0000e-04\n",
      "Epoch 706/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2632 - val_loss: 0.3282 - learning_rate: 1.0000e-04\n",
      "Epoch 707/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2659 - val_loss: 0.3282 - learning_rate: 1.0000e-04\n",
      "Epoch 708/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2715 - val_loss: 0.3282 - learning_rate: 1.0000e-04\n",
      "Epoch 709/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2735 - val_loss: 0.3282 - learning_rate: 1.0000e-04\n",
      "Epoch 710/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2667 - val_loss: 0.3282 - learning_rate: 1.0000e-04\n",
      "Epoch 711/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2679 - val_loss: 0.3282 - learning_rate: 1.0000e-04\n",
      "Epoch 712/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2822 - val_loss: 0.3282 - learning_rate: 1.0000e-04\n",
      "Epoch 713/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2618 - val_loss: 0.3282 - learning_rate: 1.0000e-04\n",
      "Epoch 714/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2619 - val_loss: 0.3282 - learning_rate: 1.0000e-04\n",
      "Epoch 715/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2822 - val_loss: 0.3282 - learning_rate: 1.0000e-04\n",
      "Epoch 716/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2788 - val_loss: 0.3282 - learning_rate: 1.0000e-04\n",
      "Epoch 717/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2796 - val_loss: 0.3282 - learning_rate: 1.0000e-04\n",
      "Epoch 718/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2839 - val_loss: 0.3282 - learning_rate: 1.0000e-04\n",
      "Epoch 719/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2636 - val_loss: 0.3282 - learning_rate: 1.0000e-04\n",
      "Epoch 720/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2649 - val_loss: 0.3281 - learning_rate: 1.0000e-04\n",
      "Epoch 721/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2697 - val_loss: 0.3282 - learning_rate: 1.0000e-04\n",
      "Epoch 722/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2782 - val_loss: 0.3282 - learning_rate: 1.0000e-04\n",
      "Epoch 723/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2703 - val_loss: 0.3282 - learning_rate: 1.0000e-04\n",
      "Epoch 724/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2823 - val_loss: 0.3282 - learning_rate: 1.0000e-04\n",
      "Epoch 725/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2704 - val_loss: 0.3282 - learning_rate: 1.0000e-04\n",
      "Epoch 726/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2706 - val_loss: 0.3282 - learning_rate: 1.0000e-04\n",
      "Epoch 727/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2686 - val_loss: 0.3282 - learning_rate: 1.0000e-04\n",
      "Epoch 728/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2814 - val_loss: 0.3282 - learning_rate: 1.0000e-04\n",
      "Epoch 729/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2904 - val_loss: 0.3283 - learning_rate: 1.0000e-04\n",
      "Epoch 730/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2823 - val_loss: 0.3282 - learning_rate: 1.0000e-04\n",
      "Epoch 731/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2728 - val_loss: 0.3282 - learning_rate: 1.0000e-04\n",
      "Epoch 732/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2844 - val_loss: 0.3281 - learning_rate: 1.0000e-04\n",
      "Epoch 733/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2836 - val_loss: 0.3281 - learning_rate: 1.0000e-04\n",
      "Epoch 734/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2758 - val_loss: 0.3281 - learning_rate: 1.0000e-04\n",
      "Epoch 735/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2672 - val_loss: 0.3281 - learning_rate: 1.0000e-04\n",
      "Epoch 736/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2776 - val_loss: 0.3281 - learning_rate: 1.0000e-04\n",
      "Epoch 737/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2801 - val_loss: 0.3281 - learning_rate: 1.0000e-04\n",
      "Epoch 738/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2780 - val_loss: 0.3281 - learning_rate: 1.0000e-04\n",
      "Epoch 739/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2781 - val_loss: 0.3281 - learning_rate: 1.0000e-04\n",
      "Epoch 740/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2804 - val_loss: 0.3281 - learning_rate: 1.0000e-04\n",
      "Epoch 741/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2789 - val_loss: 0.3281 - learning_rate: 1.0000e-04\n",
      "Epoch 742/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2648 - val_loss: 0.3281 - learning_rate: 1.0000e-04\n",
      "Epoch 743/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2780 - val_loss: 0.3281 - learning_rate: 1.0000e-04\n",
      "Epoch 744/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2642 - val_loss: 0.3281 - learning_rate: 1.0000e-04\n",
      "Epoch 745/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2643 - val_loss: 0.3281 - learning_rate: 1.0000e-04\n",
      "Epoch 746/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2623 - val_loss: 0.3281 - learning_rate: 1.0000e-04\n",
      "Epoch 747/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2733 - val_loss: 0.3281 - learning_rate: 1.0000e-04\n",
      "Epoch 748/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2648 - val_loss: 0.3281 - learning_rate: 1.0000e-04\n",
      "Epoch 749/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2667 - val_loss: 0.3281 - learning_rate: 1.0000e-04\n",
      "Epoch 750/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2849 - val_loss: 0.3281 - learning_rate: 1.0000e-04\n",
      "Epoch 751/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2779 - val_loss: 0.3281 - learning_rate: 1.0000e-04\n",
      "Epoch 752/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2722 - val_loss: 0.3281 - learning_rate: 1.0000e-04\n",
      "Epoch 753/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2646 - val_loss: 0.3281 - learning_rate: 1.0000e-04\n",
      "Epoch 754/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2742 - val_loss: 0.3280 - learning_rate: 1.0000e-04\n",
      "Epoch 755/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2712 - val_loss: 0.3280 - learning_rate: 1.0000e-04\n",
      "Epoch 756/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2662 - val_loss: 0.3280 - learning_rate: 1.0000e-04\n",
      "Epoch 757/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2904 - val_loss: 0.3280 - learning_rate: 1.0000e-04\n",
      "Epoch 758/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2785 - val_loss: 0.3280 - learning_rate: 1.0000e-04\n",
      "Epoch 759/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2741 - val_loss: 0.3280 - learning_rate: 1.0000e-04\n",
      "Epoch 760/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2652 - val_loss: 0.3280 - learning_rate: 1.0000e-04\n",
      "Epoch 761/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2670 - val_loss: 0.3280 - learning_rate: 1.0000e-04\n",
      "Epoch 762/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2701 - val_loss: 0.3280 - learning_rate: 1.0000e-04\n",
      "Epoch 763/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2654 - val_loss: 0.3280 - learning_rate: 1.0000e-04\n",
      "Epoch 764/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2693 - val_loss: 0.3280 - learning_rate: 1.0000e-04\n",
      "Epoch 765/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2784 - val_loss: 0.3280 - learning_rate: 1.0000e-04\n",
      "Epoch 766/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2757 - val_loss: 0.3280 - learning_rate: 1.0000e-04\n",
      "Epoch 767/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2782 - val_loss: 0.3280 - learning_rate: 1.0000e-04\n",
      "Epoch 768/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2674 - val_loss: 0.3280 - learning_rate: 1.0000e-04\n",
      "Epoch 769/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2764 - val_loss: 0.3280 - learning_rate: 1.0000e-04\n",
      "Epoch 770/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2787 - val_loss: 0.3281 - learning_rate: 1.0000e-04\n",
      "Epoch 771/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2690 - val_loss: 0.3280 - learning_rate: 1.0000e-04\n",
      "Epoch 772/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2744 - val_loss: 0.3281 - learning_rate: 1.0000e-04\n",
      "Epoch 773/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2595 - val_loss: 0.3281 - learning_rate: 1.0000e-04\n",
      "Epoch 774/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2660 - val_loss: 0.3281 - learning_rate: 1.0000e-04\n",
      "Epoch 775/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2621 - val_loss: 0.3281 - learning_rate: 1.0000e-04\n",
      "Epoch 776/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2799 - val_loss: 0.3281 - learning_rate: 1.0000e-04\n",
      "Epoch 777/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2715 - val_loss: 0.3281 - learning_rate: 1.0000e-04\n",
      "Epoch 778/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2777 - val_loss: 0.3281 - learning_rate: 1.0000e-04\n",
      "Epoch 779/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2709 - val_loss: 0.3281 - learning_rate: 1.0000e-04\n",
      "Epoch 780/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2610 - val_loss: 0.3281 - learning_rate: 1.0000e-04\n",
      "Epoch 781/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2688 - val_loss: 0.3281 - learning_rate: 1.0000e-04\n",
      "Epoch 782/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2691 - val_loss: 0.3280 - learning_rate: 1.0000e-04\n",
      "Epoch 783/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2771 - val_loss: 0.3280 - learning_rate: 1.0000e-04\n",
      "Epoch 784/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2717 - val_loss: 0.3281 - learning_rate: 1.0000e-04\n",
      "Epoch 785/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2806 - val_loss: 0.3281 - learning_rate: 1.0000e-04\n",
      "Epoch 786/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2734 - val_loss: 0.3281 - learning_rate: 1.0000e-04\n",
      "Epoch 787/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2699 - val_loss: 0.3281 - learning_rate: 1.0000e-04\n",
      "Epoch 788/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2769 - val_loss: 0.3280 - learning_rate: 1.0000e-04\n",
      "Epoch 789/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2748 - val_loss: 0.3281 - learning_rate: 1.0000e-04\n",
      "Epoch 790/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2740 - val_loss: 0.3281 - learning_rate: 1.0000e-04\n",
      "Epoch 791/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2808 - val_loss: 0.3281 - learning_rate: 1.0000e-04\n",
      "Epoch 792/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2649 - val_loss: 0.3281 - learning_rate: 1.0000e-04\n",
      "Epoch 793/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2681 - val_loss: 0.3280 - learning_rate: 1.0000e-04\n",
      "Epoch 794/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2810 - val_loss: 0.3281 - learning_rate: 1.0000e-04\n",
      "Epoch 795/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2862 - val_loss: 0.3281 - learning_rate: 1.0000e-04\n",
      "Epoch 796/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2743 - val_loss: 0.3280 - learning_rate: 1.0000e-04\n",
      "Epoch 797/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2762 - val_loss: 0.3281 - learning_rate: 1.0000e-04\n",
      "Epoch 798/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2710 - val_loss: 0.3281 - learning_rate: 1.0000e-04\n",
      "Epoch 799/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2624 - val_loss: 0.3281 - learning_rate: 1.0000e-04\n",
      "Epoch 800/800\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2711 - val_loss: 0.3281 - learning_rate: 1.0000e-04\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from keras import models, layers, callbacks, optimizers\n",
    "\n",
    "# Define the model with improved architecture and regularization\n",
    "model = models.Sequential([\n",
    "    layers.Input(shape=(X_train_scaled.shape[1],)), \n",
    "    layers.Dense(100, activation='relu', kernel_regularizer='l2'),\n",
    "    layers.Dropout(0.2),  \n",
    "    layers.Dense(100, activation='relu', kernel_regularizer='l2'),  \n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(200, activation='relu', kernel_regularizer='l2'),  \n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(200, activation='relu', kernel_regularizer='l2'),  \n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(100, activation='relu', kernel_regularizer='l2'),  \n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(50, activation='relu', kernel_regularizer='l2'),  \n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(15, activation='relu', kernel_regularizer='l2'),  \n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(6, activation='relu', kernel_regularizer='l2'), \n",
    "    layers.Dense(1,activation='linear')  # Output layer\n",
    "])\n",
    "\n",
    "# Compile the model with a lower learning rate and Adam optimizer\n",
    "learning_rate = 0.1\n",
    "optimizer = optimizers.SGD(learning_rate=learning_rate)\n",
    "model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "\n",
    "# Define callbacks for early stopping and learning rate reduction\n",
    "#early_stopping = callbacks.EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "reduce_lr = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.0001)\n",
    "\n",
    "# Train the model with more epochs and callbacks\n",
    "history = model.fit(X_train_scaled, y_train_top, \n",
    "                    epochs=800, \n",
    "                    validation_split=0.1, \n",
    "                    batch_size=50, \n",
    "                    verbose=1,\n",
    "                    callbacks=[ reduce_lr])\n",
    "\n",
    "# Plotting training and validation loss\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['loss'], label='train loss')\n",
    "plt.plot(history.history['val_loss'], label='val loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import GridSearchCV\n",
    "# !pip install scikeras\n",
    "# import scikeras\n",
    "# from scikeras.wrappers import KerasRegressor\n",
    "\n",
    "# # Define a function to create your Keras model\n",
    "# def create_model(learning_rate=0.001, dropout_rate=0.2, num_neurons=64, regularization_strength=0.001):\n",
    "#     model = models.Sequential([\n",
    "#         layers.Input(shape=(X_train_scaled.shape[1],)), \n",
    "#         layers.Dense(num_neurons, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(regularization_strength)),\n",
    "#         layers.Dropout(dropout_rate),  \n",
    "#         layers.Dense(num_neurons//2, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(regularization_strength)),  \n",
    "#         layers.Dropout(dropout_rate),\n",
    "#         layers.Dense(num_neurons//4, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(regularization_strength)),  \n",
    "#         layers.Dropout(dropout_rate),\n",
    "#         layers.Dense(1)  # Output layer\n",
    "#     ])\n",
    "#     optimizer = optimizers.Adam(learning_rate=learning_rate)\n",
    "#     model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "#     return model\n",
    "\n",
    "# # Create a KerasRegressor based on your Keras model\n",
    "# model = KerasRegressor(build_fn=create_model, verbose=0, dropout_rate=0.2, learning_rate=0.001, num_neurons=64, regularization_strength=0.001)\n",
    "\n",
    "# # Define the hyperparameters grid\n",
    "# param_grid = {\n",
    "#     'learning_rate': [0.001, 0.01, 0.1],\n",
    "#     'dropout_rate': [0.2, 0.3, 0.4],\n",
    "#     'num_neurons': [64, 128, 256],\n",
    "#     'regularization_strength': [0.001, 0.01, 0.1]\n",
    "# }\n",
    "\n",
    "# # Perform grid search with cross-validation\n",
    "# grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=3, verbose=1)\n",
    "# grid_result = grid.fit(X_train_scaled, y_train_top)\n",
    "\n",
    "# # Print the best parameters and corresponding score\n",
    "# print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.338601 using {'dropout_rate': 0.2, 'learning_rate': 0.01, 'num_neurons': 128, 'regularization_strength': 0.1}\n"
     ]
    }
   ],
   "source": [
    "#print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "save_model(model, 'my_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3787 \n",
      "Test Loss: 0.36931660771369934\n"
     ]
    }
   ],
   "source": [
    "\n",
    "loss = model.evaluate(X_test_scaled, y_test_top)\n",
    "print(\"Test Loss:\", loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.array(predictions).reshape(-1)  # Reshape predictions to be 1-dimensional\n",
    "y_test = np.array(y_test_top).reshape(-1)            # Reshape y_test to be 1-dimensional\n",
    "results = pd.DataFrame({'Predictions': predictions, 'Targets': y_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predictions</th>\n",
       "      <th>Targets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.804199</td>\n",
       "      <td>0.869666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.385587</td>\n",
       "      <td>0.958607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.157302</td>\n",
       "      <td>1.698970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.844843</td>\n",
       "      <td>1.060481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.208134</td>\n",
       "      <td>0.769551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>1.105001</td>\n",
       "      <td>0.974694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>1.491151</td>\n",
       "      <td>0.686133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>0.507308</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>1.672789</td>\n",
       "      <td>1.455932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>0.970246</td>\n",
       "      <td>1.244125</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>214 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Predictions   Targets\n",
       "0       0.804199  0.869666\n",
       "1       1.385587  0.958607\n",
       "2       1.157302  1.698970\n",
       "3       0.844843  1.060481\n",
       "4       1.208134  0.769551\n",
       "..           ...       ...\n",
       "209     1.105001  0.974694\n",
       "210     1.491151  0.686133\n",
       "211     0.507308  1.000000\n",
       "212     1.672789  1.455932\n",
       "213     0.970246  1.244125\n",
       "\n",
       "[214 rows x 2 columns]"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "mae = mean_absolute_error(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "r2 = r2_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.48497196077500204 0.2909224840402542\n"
     ]
    }
   ],
   "source": [
    "print(r2, mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "import structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdkit.Chem.rdchem.Mol"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a =structure.smile_to_image(\"CC\")\n",
    "type(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
