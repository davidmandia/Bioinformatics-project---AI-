{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fraction Unbound (Human)\n",
    "Description: Fraction unbound (FU) refers to the proportion of a small molecule drug that is not bound to proteins in the bloodstream of humans. FU is an important pharmacokinetic property because only the unbound fraction of a drug is typically available to exert pharmacological effects or be metabolized and eliminated from the body. Therefore, it directly influences the drug's potency, efficacy, and potential for adverse effects.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In pharmacokinetics and pharmacology, Fraction Unbound (Human), also known as fu (human), refers to the fraction of a drug that is unbound or free in the plasma. It represents the proportion of the drug that is not bound to plasma proteins and is available for distribution and pharmacological action.\n",
    "\n",
    "High Fraction Unbound (fu): A high fraction unbound indicates that a larger portion of the drug is in its free form and available for distribution to tissues and interaction with its target receptors or enzymes. This can lead to increased pharmacological activity and efficacy, as a higher concentration of the drug is present in the bloodstream and able to exert its effects.\n",
    "\n",
    "Low Fraction Unbound (fu): Conversely, a low fraction unbound suggests that a significant portion of the drug is bound to plasma proteins, reducing its availability for distribution and pharmacological action. While a low fu may increase the drug's plasma half-life and stability, it can also decrease its pharmacological activity and efficacy as less free drug is available to interact with target sites.\n",
    "\n",
    "The optimal fraction unbound for a given drug depends on various factors, including its pharmacokinetic and pharmacodynamic properties, therapeutic index, and desired clinical outcomes. Therefore, the significance of the fraction unbound in drug therapy depends on the specific context and the therapeutic goals of the treatment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the information provided, here are some strategies to potentially improve your model:\n",
    "\n",
    "### 1. Feature Engineering:\n",
    "- **Feature Selection**: Analyze the importance of each feature and consider removing irrelevant or redundant ones. You have a large number of features, so feature selection techniques like Recursive Feature Elimination (RFE) or feature importance from a tree-based model could be beneficial.\n",
    "- **Feature Scaling**: Ensure all features are scaled appropriately, especially if they have different scales.\n",
    "\n",
    "### 2. Model Architecture:\n",
    "- **Simplification**: The current model architecture is quite complex, which may lead to overfitting, especially given the large number of features. Consider reducing the number of layers or neurons to simplify the model.\n",
    "- **Regularization**: Regularization techniques like dropout and L2 regularization can help prevent overfitting. However, you should carefully tune the dropout rate and regularization strength to find the optimal balance.\n",
    "\n",
    "### 3. Hyperparameter Tuning:\n",
    "- **Learning Rate**: Experiment with different learning rates to find the one that results in faster convergence without oscillating or diverging.\n",
    "- **Batch Size**: Adjust the batch size and monitor the training dynamics. Smaller batch sizes often generalize better but may result in slower convergence.\n",
    "- **Optimizer**: Besides Adam, try other optimizers such as RMSprop or SGD with momentum, and tune their parameters accordingly.\n",
    "\n",
    "### 4. Training Strategy:\n",
    "- **Early Stopping**: The `EarlyStopping` callback is already included, but you can further tune its parameters like the `patience` to stop training at the right moment.\n",
    "- **Learning Rate Scheduler**: Instead of a fixed learning rate, consider using a learning rate scheduler to dynamically adjust the learning rate during training.\n",
    "\n",
    "### 5. Cross-Validation:\n",
    "- Perform k-fold cross-validation to assess the model's stability and generalization performance across different subsets of the data.\n",
    "\n",
    "### 6. Data Augmentation:\n",
    "- If you have limited data, consider applying data augmentation techniques to artificially increase the size of your training set.\n",
    "\n",
    "### 7. Model Evaluation:\n",
    "- Besides the test loss, evaluate the model on other metrics relevant to your problem, such as accuracy, precision, recall, or F1 score, especially if your problem is a classification task.\n",
    "\n",
    "### Example Workflow:\n",
    "1. **Feature Selection**: Identify the most important features using feature importance techniques.\n",
    "2. **Model Simplification**: Reduce the complexity of the model architecture.\n",
    "3. **Hyperparameter Tuning**: Tune hyperparameters using techniques like grid search or random search.\n",
    "4. **Cross-Validation**: Assess model performance using k-fold cross-validation.\n",
    "5. **Evaluation**: Evaluate the model on multiple metrics and compare with baseline models or other algorithms.\n",
    "\n",
    "By systematically going through these steps and experimenting with different configurations, you should be able to iteratively improve your model's performance. Remember to keep track of your experiments and document your findings to make informed decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rdkit in /usr/local/python/3.10.13/lib/python3.10/site-packages (2023.9.6)\n",
      "Requirement already satisfied: numpy in /home/codespace/.local/lib/python3.10/site-packages (from rdkit) (1.26.4)\n",
      "Requirement already satisfied: Pillow in /home/codespace/.local/lib/python3.10/site-packages (from rdkit) (10.3.0)\n",
      "Collecting Sklearn\n",
      "  Using cached sklearn-0.0.post12.tar.gz (2.6 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[15 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m The 'sklearn' PyPI package is deprecated, use 'scikit-learn'\n",
      "  \u001b[31m   \u001b[0m rather than 'sklearn' for pip commands.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m Here is how to fix this error in the main use cases:\n",
      "  \u001b[31m   \u001b[0m - use 'pip install scikit-learn' rather than 'pip install sklearn'\n",
      "  \u001b[31m   \u001b[0m - replace 'sklearn' by 'scikit-learn' in your pip requirements files\n",
      "  \u001b[31m   \u001b[0m   (requirements.txt, setup.py, setup.cfg, Pipfile, etc ...)\n",
      "  \u001b[31m   \u001b[0m - if the 'sklearn' package is used by one of your dependencies,\n",
      "  \u001b[31m   \u001b[0m   it would be great if you take some time to track which package uses\n",
      "  \u001b[31m   \u001b[0m   'sklearn' instead of 'scikit-learn' and report it to their issue tracker\n",
      "  \u001b[31m   \u001b[0m - as a last resort, set the environment variable\n",
      "  \u001b[31m   \u001b[0m   SKLEARN_ALLOW_DEPRECATED_SKLEARN_PACKAGE_INSTALL=True to avoid this error\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m More information is available at\n",
      "  \u001b[31m   \u001b[0m https://github.com/scikit-learn/sklearn-pypi-package\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[?25h\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
      "\u001b[31m╰─>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
      "\u001b[1;36mhint\u001b[0m: See above for details.\n",
      "Requirement already satisfied: tensorflow in /usr/local/python/3.10.13/lib/python3.10/site-packages (2.16.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from tensorflow) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from tensorflow) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from tensorflow) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in /home/codespace/.local/lib/python3.10/site-packages (from tensorflow) (24.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from tensorflow) (4.25.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/codespace/.local/lib/python3.10/site-packages (from tensorflow) (2.31.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/python/3.10.13/lib/python3.10/site-packages (from tensorflow) (68.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/codespace/.local/lib/python3.10/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/codespace/.local/lib/python3.10/site-packages (from tensorflow) (4.10.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from tensorflow) (1.63.0)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from tensorflow) (2.16.2)\n",
      "Requirement already satisfied: keras>=3.0.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from tensorflow) (3.3.3)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from tensorflow) (0.37.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /home/codespace/.local/lib/python3.10/site-packages (from tensorflow) (1.26.4)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
      "Requirement already satisfied: rich in /usr/local/python/3.10.13/lib/python3.10/site-packages (from keras>=3.0.0->tensorflow) (13.7.1)\n",
      "Requirement already satisfied: namex in /usr/local/python/3.10.13/lib/python3.10/site-packages (from keras>=3.0.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in /usr/local/python/3.10.13/lib/python3.10/site-packages (from keras>=3.0.0->tensorflow) (0.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.2.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/codespace/.local/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow) (2.1.5)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from rich->keras>=3.0.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/codespace/.local/lib/python3.10/site-packages (from rich->keras>=3.0.0->tensorflow) (2.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow) (0.1.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 20:57:39.424773: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-05-16 20:57:40.506447: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-05-16 20:57:42.854769: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-16 20:57:46.330022: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "!pip install rdkit\n",
    "!pip install Sklearn\n",
    "!pip install tensorflow\n",
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from keras.models import save_model\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fu = pd.read_csv(\"data/fu_train.csv\", header=0)\n",
    "test = pd.read_csv(\"data/fu_test.csv\", header=0)\n",
    "data_fu= pd.concat([data_fu, test])\n",
    "\n",
    "data_fu.columns = ['smiles', 'label', 'group']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fu['Molecule'] = data_fu['smiles'].apply(Chem.MolFromSmiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2139, 4)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_fu.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidIndexError",
     "evalue": "Reindexing only valid with uniquely valued Index objects",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidIndexError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m descriptor_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(all_descriptors\u001b[38;5;241m.\u001b[39mtolist())\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Concatenate the original dataframe with the descriptor dataframe\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m data_fu_descriptor \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdata_fu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdescriptor_df\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/reshape/concat.py:395\u001b[0m, in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    380\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    382\u001b[0m op \u001b[38;5;241m=\u001b[39m _Concatenator(\n\u001b[1;32m    383\u001b[0m     objs,\n\u001b[1;32m    384\u001b[0m     axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    392\u001b[0m     sort\u001b[38;5;241m=\u001b[39msort,\n\u001b[1;32m    393\u001b[0m )\n\u001b[0;32m--> 395\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/reshape/concat.py:680\u001b[0m, in \u001b[0;36m_Concatenator.get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    678\u001b[0m         obj_labels \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39maxes[\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m ax]\n\u001b[1;32m    679\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m new_labels\u001b[38;5;241m.\u001b[39mequals(obj_labels):\n\u001b[0;32m--> 680\u001b[0m             indexers[ax] \u001b[38;5;241m=\u001b[39m \u001b[43mobj_labels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    682\u001b[0m     mgrs_indexers\u001b[38;5;241m.\u001b[39mappend((obj\u001b[38;5;241m.\u001b[39m_mgr, indexers))\n\u001b[1;32m    684\u001b[0m new_data \u001b[38;5;241m=\u001b[39m concatenate_managers(\n\u001b[1;32m    685\u001b[0m     mgrs_indexers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnew_axes, concat_axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbm_axis, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy\n\u001b[1;32m    686\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/indexes/base.py:3885\u001b[0m, in \u001b[0;36mIndex.get_indexer\u001b[0;34m(self, target, method, limit, tolerance)\u001b[0m\n\u001b[1;32m   3882\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_method(method, limit, tolerance)\n\u001b[1;32m   3884\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_as_unique:\n\u001b[0;32m-> 3885\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_requires_unique_msg)\n\u001b[1;32m   3887\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(target) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   3888\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([], dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mintp)\n",
      "\u001b[0;31mInvalidIndexError\u001b[0m: Reindexing only valid with uniquely valued Index objects"
     ]
    }
   ],
   "source": [
    "from rdkit.Chem import Descriptors, AllChem\n",
    "# Function to calculate all molecular descriptors for a molecule\n",
    "def calculate_all_descriptors(molecule):\n",
    "    descriptors = {}\n",
    "    for descriptor, descriptor_fn in Descriptors.descList:\n",
    "        descriptors[descriptor] = descriptor_fn(molecule)\n",
    "    return descriptors\n",
    "\n",
    "# Calculate all molecular descriptors for each molecule\n",
    "all_descriptors = data_fu['Molecule'].apply(calculate_all_descriptors)\n",
    "\n",
    "# Convert dictionary of descriptors into dataframe\n",
    "descriptor_df = pd.DataFrame(all_descriptors.tolist())\n",
    "\n",
    "# Concatenate the original dataframe with the descriptor dataframe\n",
    "data_fu_descriptor = pd.concat([data_fu, descriptor_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_fu_descriptor.columns[data_fu_descriptor.isna().any()].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_desc =  [descr[0] for descr in Descriptors.descList]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_fu_descriptor[list_desc].values\n",
    "y = data_fu_descriptor['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# Trying without scaler to capture variability \n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1520, 210)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 3.0971 - val_loss: 2.7622 - learning_rate: 0.0010\n",
      "Epoch 2/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.3210 - val_loss: 2.5117 - learning_rate: 0.0010\n",
      "Epoch 3/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.2042 - val_loss: 2.1847 - learning_rate: 0.0010\n",
      "Epoch 4/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.0646 - val_loss: 2.1021 - learning_rate: 0.0010\n",
      "Epoch 5/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.9523 - val_loss: 2.0102 - learning_rate: 0.0010\n",
      "Epoch 6/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8951 - val_loss: 1.9952 - learning_rate: 0.0010\n",
      "Epoch 7/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.8514 - val_loss: 1.9454 - learning_rate: 0.0010\n",
      "Epoch 8/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.8052 - val_loss: 1.9184 - learning_rate: 0.0010\n",
      "Epoch 9/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7435 - val_loss: 1.8879 - learning_rate: 0.0010\n",
      "Epoch 10/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.7112 - val_loss: 1.7887 - learning_rate: 0.0010\n",
      "Epoch 11/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.6439 - val_loss: 1.7511 - learning_rate: 0.0010\n",
      "Epoch 12/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.5575 - val_loss: 1.7489 - learning_rate: 0.0010\n",
      "Epoch 13/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.6376 - val_loss: 1.6930 - learning_rate: 0.0010\n",
      "Epoch 14/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.5547 - val_loss: 1.6965 - learning_rate: 0.0010\n",
      "Epoch 15/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.4995 - val_loss: 1.5613 - learning_rate: 0.0010\n",
      "Epoch 16/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.4430 - val_loss: 1.5557 - learning_rate: 0.0010\n",
      "Epoch 17/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.3838 - val_loss: 1.5656 - learning_rate: 0.0010\n",
      "Epoch 18/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.3509 - val_loss: 1.5267 - learning_rate: 0.0010\n",
      "Epoch 19/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.3371 - val_loss: 1.4418 - learning_rate: 0.0010\n",
      "Epoch 20/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.2520 - val_loss: 1.4785 - learning_rate: 0.0010\n",
      "Epoch 21/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.2602 - val_loss: 1.4263 - learning_rate: 0.0010\n",
      "Epoch 22/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.2185 - val_loss: 1.2731 - learning_rate: 0.0010\n",
      "Epoch 23/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1647 - val_loss: 1.2629 - learning_rate: 0.0010\n",
      "Epoch 24/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1395 - val_loss: 1.2319 - learning_rate: 0.0010\n",
      "Epoch 25/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0986 - val_loss: 1.1810 - learning_rate: 0.0010\n",
      "Epoch 26/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0735 - val_loss: 1.2116 - learning_rate: 0.0010\n",
      "Epoch 27/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.0398 - val_loss: 1.1615 - learning_rate: 0.0010\n",
      "Epoch 28/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0016 - val_loss: 1.1008 - learning_rate: 0.0010\n",
      "Epoch 29/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9542 - val_loss: 1.0990 - learning_rate: 0.0010\n",
      "Epoch 30/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9480 - val_loss: 1.1181 - learning_rate: 0.0010\n",
      "Epoch 31/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.9203 - val_loss: 1.0342 - learning_rate: 0.0010\n",
      "Epoch 32/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.8793 - val_loss: 0.9826 - learning_rate: 0.0010\n",
      "Epoch 33/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.8692 - val_loss: 1.0062 - learning_rate: 0.0010\n",
      "Epoch 34/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.8473 - val_loss: 0.9381 - learning_rate: 0.0010\n",
      "Epoch 35/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.8084 - val_loss: 0.8969 - learning_rate: 0.0010\n",
      "Epoch 36/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7883 - val_loss: 0.8784 - learning_rate: 0.0010\n",
      "Epoch 37/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.7378 - val_loss: 0.8712 - learning_rate: 0.0010\n",
      "Epoch 38/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7327 - val_loss: 0.8358 - learning_rate: 0.0010\n",
      "Epoch 39/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7144 - val_loss: 0.8444 - learning_rate: 0.0010\n",
      "Epoch 40/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6808 - val_loss: 0.7668 - learning_rate: 0.0010\n",
      "Epoch 41/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6583 - val_loss: 0.7896 - learning_rate: 0.0010\n",
      "Epoch 42/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6595 - val_loss: 0.7710 - learning_rate: 0.0010\n",
      "Epoch 43/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6260 - val_loss: 0.7543 - learning_rate: 0.0010\n",
      "Epoch 44/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6204 - val_loss: 0.7531 - learning_rate: 0.0010\n",
      "Epoch 45/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5948 - val_loss: 0.7335 - learning_rate: 0.0010\n",
      "Epoch 46/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5892 - val_loss: 0.6747 - learning_rate: 0.0010\n",
      "Epoch 47/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5409 - val_loss: 0.6662 - learning_rate: 0.0010\n",
      "Epoch 48/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5362 - val_loss: 0.6327 - learning_rate: 0.0010\n",
      "Epoch 49/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5266 - val_loss: 0.6566 - learning_rate: 0.0010\n",
      "Epoch 50/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5102 - val_loss: 0.6060 - learning_rate: 0.0010\n",
      "Epoch 51/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5082 - val_loss: 0.6077 - learning_rate: 0.0010\n",
      "Epoch 52/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4780 - val_loss: 0.5760 - learning_rate: 0.0010\n",
      "Epoch 53/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4776 - val_loss: 0.5659 - learning_rate: 0.0010\n",
      "Epoch 54/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4556 - val_loss: 0.5665 - learning_rate: 0.0010\n",
      "Epoch 55/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4396 - val_loss: 0.5446 - learning_rate: 0.0010\n",
      "Epoch 56/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4422 - val_loss: 0.5593 - learning_rate: 0.0010\n",
      "Epoch 57/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4442 - val_loss: 0.5327 - learning_rate: 0.0010\n",
      "Epoch 58/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4051 - val_loss: 0.5329 - learning_rate: 0.0010\n",
      "Epoch 59/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3953 - val_loss: 0.5122 - learning_rate: 0.0010\n",
      "Epoch 60/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3800 - val_loss: 0.4993 - learning_rate: 0.0010\n",
      "Epoch 61/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3648 - val_loss: 0.4944 - learning_rate: 0.0010\n",
      "Epoch 62/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3741 - val_loss: 0.4858 - learning_rate: 0.0010\n",
      "Epoch 63/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3711 - val_loss: 0.4701 - learning_rate: 0.0010\n",
      "Epoch 64/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3448 - val_loss: 0.4705 - learning_rate: 0.0010\n",
      "Epoch 65/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3501 - val_loss: 0.4738 - learning_rate: 0.0010\n",
      "Epoch 66/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3481 - val_loss: 0.4517 - learning_rate: 0.0010\n",
      "Epoch 67/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3212 - val_loss: 0.4512 - learning_rate: 0.0010\n",
      "Epoch 68/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3272 - val_loss: 0.4390 - learning_rate: 0.0010\n",
      "Epoch 69/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3000 - val_loss: 0.4394 - learning_rate: 0.0010\n",
      "Epoch 70/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3002 - val_loss: 0.4335 - learning_rate: 0.0010\n",
      "Epoch 71/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3090 - val_loss: 0.4326 - learning_rate: 0.0010\n",
      "Epoch 72/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3095 - val_loss: 0.4197 - learning_rate: 0.0010\n",
      "Epoch 73/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2895 - val_loss: 0.4201 - learning_rate: 0.0010\n",
      "Epoch 74/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3015 - val_loss: 0.4171 - learning_rate: 0.0010\n",
      "Epoch 75/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2997 - val_loss: 0.4147 - learning_rate: 0.0010\n",
      "Epoch 76/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2875 - val_loss: 0.4020 - learning_rate: 0.0010\n",
      "Epoch 77/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2771 - val_loss: 0.4096 - learning_rate: 0.0010\n",
      "Epoch 78/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2756 - val_loss: 0.4119 - learning_rate: 0.0010\n",
      "Epoch 79/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2785 - val_loss: 0.4066 - learning_rate: 0.0010\n",
      "Epoch 80/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2644 - val_loss: 0.4042 - learning_rate: 0.0010\n",
      "Epoch 81/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2620 - val_loss: 0.3966 - learning_rate: 0.0010\n",
      "Epoch 82/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2537 - val_loss: 0.3803 - learning_rate: 0.0010\n",
      "Epoch 83/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2482 - val_loss: 0.3848 - learning_rate: 0.0010\n",
      "Epoch 84/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2441 - val_loss: 0.3953 - learning_rate: 0.0010\n",
      "Epoch 85/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2639 - val_loss: 0.4026 - learning_rate: 0.0010\n",
      "Epoch 86/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2502 - val_loss: 0.3812 - learning_rate: 0.0010\n",
      "Epoch 87/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2458 - val_loss: 0.3979 - learning_rate: 0.0010\n",
      "Epoch 88/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2403 - val_loss: 0.3734 - learning_rate: 2.0000e-04\n",
      "Epoch 89/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2207 - val_loss: 0.3708 - learning_rate: 2.0000e-04\n",
      "Epoch 90/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2242 - val_loss: 0.3717 - learning_rate: 2.0000e-04\n",
      "Epoch 91/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2084 - val_loss: 0.3621 - learning_rate: 2.0000e-04\n",
      "Epoch 92/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2118 - val_loss: 0.3634 - learning_rate: 2.0000e-04\n",
      "Epoch 93/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2216 - val_loss: 0.3603 - learning_rate: 2.0000e-04\n",
      "Epoch 94/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2019 - val_loss: 0.3580 - learning_rate: 2.0000e-04\n",
      "Epoch 95/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1949 - val_loss: 0.3612 - learning_rate: 2.0000e-04\n",
      "Epoch 96/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2012 - val_loss: 0.3564 - learning_rate: 2.0000e-04\n",
      "Epoch 97/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2035 - val_loss: 0.3602 - learning_rate: 2.0000e-04\n",
      "Epoch 98/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1923 - val_loss: 0.3519 - learning_rate: 2.0000e-04\n",
      "Epoch 99/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1823 - val_loss: 0.3459 - learning_rate: 2.0000e-04\n",
      "Epoch 100/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1869 - val_loss: 0.3524 - learning_rate: 2.0000e-04\n",
      "Epoch 101/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1914 - val_loss: 0.3472 - learning_rate: 2.0000e-04\n",
      "Epoch 102/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1912 - val_loss: 0.3478 - learning_rate: 2.0000e-04\n",
      "Epoch 103/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1854 - val_loss: 0.3501 - learning_rate: 2.0000e-04\n",
      "Epoch 104/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1773 - val_loss: 0.3478 - learning_rate: 2.0000e-04\n",
      "Epoch 105/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1677 - val_loss: 0.3480 - learning_rate: 1.0000e-04\n",
      "Epoch 106/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1822 - val_loss: 0.3440 - learning_rate: 1.0000e-04\n",
      "Epoch 107/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1736 - val_loss: 0.3433 - learning_rate: 1.0000e-04\n",
      "Epoch 108/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1615 - val_loss: 0.3427 - learning_rate: 1.0000e-04\n",
      "Epoch 109/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1712 - val_loss: 0.3435 - learning_rate: 1.0000e-04\n",
      "Epoch 110/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1663 - val_loss: 0.3431 - learning_rate: 1.0000e-04\n",
      "Epoch 111/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1724 - val_loss: 0.3415 - learning_rate: 1.0000e-04\n",
      "Epoch 112/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1712 - val_loss: 0.3395 - learning_rate: 1.0000e-04\n",
      "Epoch 113/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1770 - val_loss: 0.3403 - learning_rate: 1.0000e-04\n",
      "Epoch 114/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1612 - val_loss: 0.3390 - learning_rate: 1.0000e-04\n",
      "Epoch 115/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1648 - val_loss: 0.3395 - learning_rate: 1.0000e-04\n",
      "Epoch 116/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1661 - val_loss: 0.3366 - learning_rate: 1.0000e-04\n",
      "Epoch 117/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1623 - val_loss: 0.3389 - learning_rate: 1.0000e-04\n",
      "Epoch 118/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1680 - val_loss: 0.3348 - learning_rate: 1.0000e-04\n",
      "Epoch 119/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1557 - val_loss: 0.3368 - learning_rate: 1.0000e-04\n",
      "Epoch 120/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1596 - val_loss: 0.3346 - learning_rate: 1.0000e-04\n",
      "Epoch 121/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1615 - val_loss: 0.3346 - learning_rate: 1.0000e-04\n",
      "Epoch 122/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1554 - val_loss: 0.3320 - learning_rate: 1.0000e-04\n",
      "Epoch 123/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1489 - val_loss: 0.3310 - learning_rate: 1.0000e-04\n",
      "Epoch 124/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1528 - val_loss: 0.3316 - learning_rate: 1.0000e-04\n",
      "Epoch 125/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1532 - val_loss: 0.3357 - learning_rate: 1.0000e-04\n",
      "Epoch 126/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1578 - val_loss: 0.3305 - learning_rate: 1.0000e-04\n",
      "Epoch 127/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1591 - val_loss: 0.3345 - learning_rate: 1.0000e-04\n",
      "Epoch 128/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1556 - val_loss: 0.3329 - learning_rate: 1.0000e-04\n",
      "Epoch 129/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1583 - val_loss: 0.3345 - learning_rate: 1.0000e-04\n",
      "Epoch 130/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1503 - val_loss: 0.3261 - learning_rate: 1.0000e-04\n",
      "Epoch 131/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1597 - val_loss: 0.3333 - learning_rate: 1.0000e-04\n",
      "Epoch 132/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1476 - val_loss: 0.3306 - learning_rate: 1.0000e-04\n",
      "Epoch 133/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1473 - val_loss: 0.3221 - learning_rate: 1.0000e-04\n",
      "Epoch 134/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1457 - val_loss: 0.3290 - learning_rate: 1.0000e-04\n",
      "Epoch 135/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1490 - val_loss: 0.3260 - learning_rate: 1.0000e-04\n",
      "Epoch 136/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1432 - val_loss: 0.3260 - learning_rate: 1.0000e-04\n",
      "Epoch 137/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1510 - val_loss: 0.3303 - learning_rate: 1.0000e-04\n",
      "Epoch 138/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1451 - val_loss: 0.3275 - learning_rate: 1.0000e-04\n",
      "Epoch 139/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1429 - val_loss: 0.3290 - learning_rate: 1.0000e-04\n",
      "Epoch 140/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1447 - val_loss: 0.3260 - learning_rate: 1.0000e-04\n",
      "Epoch 141/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1446 - val_loss: 0.3295 - learning_rate: 1.0000e-04\n",
      "Epoch 142/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1439 - val_loss: 0.3222 - learning_rate: 1.0000e-04\n",
      "Epoch 143/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1420 - val_loss: 0.3292 - learning_rate: 1.0000e-04\n",
      "Epoch 144/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1367 - val_loss: 0.3210 - learning_rate: 1.0000e-04\n",
      "Epoch 145/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1449 - val_loss: 0.3192 - learning_rate: 1.0000e-04\n",
      "Epoch 146/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1426 - val_loss: 0.3250 - learning_rate: 1.0000e-04\n",
      "Epoch 147/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1382 - val_loss: 0.3244 - learning_rate: 1.0000e-04\n",
      "Epoch 148/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1403 - val_loss: 0.3231 - learning_rate: 1.0000e-04\n",
      "Epoch 149/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1357 - val_loss: 0.3211 - learning_rate: 1.0000e-04\n",
      "Epoch 150/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1303 - val_loss: 0.3216 - learning_rate: 1.0000e-04\n",
      "Epoch 151/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1366 - val_loss: 0.3217 - learning_rate: 1.0000e-04\n",
      "Epoch 152/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1399 - val_loss: 0.3224 - learning_rate: 1.0000e-04\n",
      "Epoch 153/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1282 - val_loss: 0.3183 - learning_rate: 1.0000e-04\n",
      "Epoch 154/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1312 - val_loss: 0.3148 - learning_rate: 1.0000e-04\n",
      "Epoch 155/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1335 - val_loss: 0.3152 - learning_rate: 1.0000e-04\n",
      "Epoch 156/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1280 - val_loss: 0.3150 - learning_rate: 1.0000e-04\n",
      "Epoch 157/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1359 - val_loss: 0.3197 - learning_rate: 1.0000e-04\n",
      "Epoch 158/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1385 - val_loss: 0.3149 - learning_rate: 1.0000e-04\n",
      "Epoch 159/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1362 - val_loss: 0.3106 - learning_rate: 1.0000e-04\n",
      "Epoch 160/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1332 - val_loss: 0.3103 - learning_rate: 1.0000e-04\n",
      "Epoch 161/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1256 - val_loss: 0.3157 - learning_rate: 1.0000e-04\n",
      "Epoch 162/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1288 - val_loss: 0.3123 - learning_rate: 1.0000e-04\n",
      "Epoch 163/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1217 - val_loss: 0.3099 - learning_rate: 1.0000e-04\n",
      "Epoch 164/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1296 - val_loss: 0.3107 - learning_rate: 1.0000e-04\n",
      "Epoch 165/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1243 - val_loss: 0.3153 - learning_rate: 1.0000e-04\n",
      "Epoch 166/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1199 - val_loss: 0.3088 - learning_rate: 1.0000e-04\n",
      "Epoch 167/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1284 - val_loss: 0.3094 - learning_rate: 1.0000e-04\n",
      "Epoch 168/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1193 - val_loss: 0.3111 - learning_rate: 1.0000e-04\n",
      "Epoch 169/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1200 - val_loss: 0.3100 - learning_rate: 1.0000e-04\n",
      "Epoch 170/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1180 - val_loss: 0.3122 - learning_rate: 1.0000e-04\n",
      "Epoch 171/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1196 - val_loss: 0.3100 - learning_rate: 1.0000e-04\n",
      "Epoch 172/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1199 - val_loss: 0.3077 - learning_rate: 1.0000e-04\n",
      "Epoch 173/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1174 - val_loss: 0.3087 - learning_rate: 1.0000e-04\n",
      "Epoch 174/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1219 - val_loss: 0.3033 - learning_rate: 1.0000e-04\n",
      "Epoch 175/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1242 - val_loss: 0.3090 - learning_rate: 1.0000e-04\n",
      "Epoch 176/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1267 - val_loss: 0.3088 - learning_rate: 1.0000e-04\n",
      "Epoch 177/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1182 - val_loss: 0.3112 - learning_rate: 1.0000e-04\n",
      "Epoch 178/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1164 - val_loss: 0.3075 - learning_rate: 1.0000e-04\n",
      "Epoch 179/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1158 - val_loss: 0.3065 - learning_rate: 1.0000e-04\n",
      "Epoch 180/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1231 - val_loss: 0.3059 - learning_rate: 1.0000e-04\n",
      "Epoch 181/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1196 - val_loss: 0.3081 - learning_rate: 1.0000e-04\n",
      "Epoch 182/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1091 - val_loss: 0.3083 - learning_rate: 1.0000e-04\n",
      "Epoch 183/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1159 - val_loss: 0.3076 - learning_rate: 1.0000e-04\n",
      "Epoch 184/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1199 - val_loss: 0.3072 - learning_rate: 1.0000e-04\n",
      "Epoch 185/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1172 - val_loss: 0.3056 - learning_rate: 1.0000e-04\n",
      "Epoch 186/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1139 - val_loss: 0.3069 - learning_rate: 1.0000e-04\n",
      "Epoch 187/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1128 - val_loss: 0.3061 - learning_rate: 1.0000e-04\n",
      "Epoch 188/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1183 - val_loss: 0.3083 - learning_rate: 1.0000e-04\n",
      "Epoch 189/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1155 - val_loss: 0.3102 - learning_rate: 1.0000e-04\n",
      "Epoch 190/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1105 - val_loss: 0.3051 - learning_rate: 1.0000e-04\n",
      "Epoch 191/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1146 - val_loss: 0.3041 - learning_rate: 1.0000e-04\n",
      "Epoch 192/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1063 - val_loss: 0.3070 - learning_rate: 1.0000e-04\n",
      "Epoch 193/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1116 - val_loss: 0.3067 - learning_rate: 1.0000e-04\n",
      "Epoch 194/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1094 - val_loss: 0.3061 - learning_rate: 1.0000e-04\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from keras import optimizers, callbacks\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.Dense(200, activation='relu', input_shape=(X_train_scaled.shape[1],), kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
    "    layers.Dropout(0.5),  # Regularization\n",
    "    layers.Dense(400, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),  # Increased complexity\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(400, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),  # Increased complexity\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(200, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),  # Increased complexity\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(200, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(100, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(50, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
    "    layers.Dense(1)  # Output layer\n",
    "])\n",
    "\n",
    "# Compile the model with a lower learning rate\n",
    "learning_rate = 0.001\n",
    "optimizer = optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "\n",
    "# Define callbacks for early stopping and learning rate reduction\n",
    "early_stopping = callbacks.EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "reduce_lr = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.0001)\n",
    "\n",
    "# Train the model with more epochs and callbacks\n",
    "history = model.fit(X_train_scaled, y_train, \n",
    "                    epochs=500, \n",
    "                    validation_split=0.3, \n",
    "                    batch_size=15, \n",
    "                    verbose=1, \n",
    "                    callbacks=[early_stopping, reduce_lr])\n",
    "\n",
    "# Plotting training and validation loss\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['loss'], label='train loss')\n",
    "plt.plot(history.history['val_loss'], label='val loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 1.4385 - val_loss: 1.2733\n",
      "Epoch 2/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.8360 - val_loss: 1.0728\n",
      "Epoch 3/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6297 - val_loss: 0.7046\n",
      "Epoch 4/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5753 - val_loss: 0.7480\n",
      "Epoch 5/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5209 - val_loss: 0.5691\n",
      "Epoch 6/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4985 - val_loss: 0.4391\n",
      "Epoch 7/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4336 - val_loss: 0.4754\n",
      "Epoch 8/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4133 - val_loss: 0.4163\n",
      "Epoch 9/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4298 - val_loss: 0.4690\n",
      "Epoch 10/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3885 - val_loss: 0.4016\n",
      "Epoch 11/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4124 - val_loss: 0.3467\n",
      "Epoch 12/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3850 - val_loss: 0.3554\n",
      "Epoch 13/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3524 - val_loss: 0.3309\n",
      "Epoch 14/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3614 - val_loss: 0.3213\n",
      "Epoch 15/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3354 - val_loss: 0.3209\n",
      "Epoch 16/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3589 - val_loss: 0.3538\n",
      "Epoch 17/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3157 - val_loss: 0.3227\n",
      "Epoch 18/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3541 - val_loss: 0.3313\n",
      "Epoch 19/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3223 - val_loss: 0.3291\n",
      "Epoch 20/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3554 - val_loss: 0.2988\n",
      "Epoch 21/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3443 - val_loss: 0.2889\n",
      "Epoch 22/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3318 - val_loss: 0.2952\n",
      "Epoch 23/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3180 - val_loss: 0.3073\n",
      "Epoch 24/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3305 - val_loss: 0.3158\n",
      "Epoch 25/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3187 - val_loss: 0.2940\n",
      "Epoch 26/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3003 - val_loss: 0.3035\n",
      "Epoch 27/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3394 - val_loss: 0.2912\n",
      "Epoch 28/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3294 - val_loss: 0.2786\n",
      "Epoch 29/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3355 - val_loss: 0.2986\n",
      "Epoch 30/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3116 - val_loss: 0.3072\n",
      "Epoch 31/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3144 - val_loss: 0.2987\n",
      "Epoch 32/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2878 - val_loss: 0.3034\n",
      "Epoch 33/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2829 - val_loss: 0.2833\n",
      "Epoch 34/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2741 - val_loss: 0.2841\n",
      "Epoch 35/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3342 - val_loss: 0.2850\n",
      "Epoch 36/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3134 - val_loss: 0.2710\n",
      "Epoch 37/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3112 - val_loss: 0.2933\n",
      "Epoch 38/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2780 - val_loss: 0.2676\n",
      "Epoch 39/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3072 - val_loss: 0.2691\n",
      "Epoch 40/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2818 - val_loss: 0.2793\n",
      "Epoch 41/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2896 - val_loss: 0.2763\n",
      "Epoch 42/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2853 - val_loss: 0.2807\n",
      "Epoch 43/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2866 - val_loss: 0.2833\n",
      "Epoch 44/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2973 - val_loss: 0.2755\n",
      "Epoch 45/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2543 - val_loss: 0.2664\n",
      "Epoch 46/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3007 - val_loss: 0.2797\n",
      "Epoch 47/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3328 - val_loss: 0.3063\n",
      "Epoch 48/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2860 - val_loss: 0.2692\n",
      "Epoch 49/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2913 - val_loss: 0.2610\n",
      "Epoch 50/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2810 - val_loss: 0.2805\n",
      "Epoch 51/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2756 - val_loss: 0.2677\n",
      "Epoch 52/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2999 - val_loss: 0.2845\n",
      "Epoch 53/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2883 - val_loss: 0.2732\n",
      "Epoch 54/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2824 - val_loss: 0.2669\n",
      "Epoch 55/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2764 - val_loss: 0.2792\n",
      "Epoch 56/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2599 - val_loss: 0.2774\n",
      "Epoch 57/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2752 - val_loss: 0.2734\n",
      "Epoch 58/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2883 - val_loss: 0.2722\n",
      "Epoch 59/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2878 - val_loss: 0.2923\n",
      "Epoch 60/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2789 - val_loss: 0.2993\n",
      "Epoch 61/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3053 - val_loss: 0.2787\n",
      "Epoch 62/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2857 - val_loss: 0.2801\n",
      "Epoch 63/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2783 - val_loss: 0.2975\n",
      "Epoch 64/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2849 - val_loss: 0.2720\n",
      "Epoch 65/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3090 - val_loss: 0.2734\n",
      "Epoch 66/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2652 - val_loss: 0.2725\n",
      "Epoch 67/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2674 - val_loss: 0.2730\n",
      "Epoch 68/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2592 - val_loss: 0.2771\n",
      "Epoch 69/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2550 - val_loss: 0.2659\n",
      "Epoch 70/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2468 - val_loss: 0.2746\n",
      "Epoch 71/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2620 - val_loss: 0.2959\n",
      "Epoch 72/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2494 - val_loss: 0.2699\n",
      "Epoch 73/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2817 - val_loss: 0.3021\n",
      "Epoch 74/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2873 - val_loss: 0.2640\n",
      "Epoch 75/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2782 - val_loss: 0.2854\n",
      "Epoch 76/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2613 - val_loss: 0.2709\n",
      "Epoch 77/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2808 - val_loss: 0.2838\n",
      "Epoch 78/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2598 - val_loss: 0.2780\n",
      "Epoch 79/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2638 - val_loss: 0.2755\n",
      "Epoch 80/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2591 - val_loss: 0.2749\n",
      "Epoch 81/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2702 - val_loss: 0.2891\n",
      "Epoch 82/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2795 - val_loss: 0.2777\n",
      "Epoch 83/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2529 - val_loss: 0.3061\n",
      "Epoch 84/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2819 - val_loss: 0.2664\n",
      "Epoch 85/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2539 - val_loss: 0.2855\n",
      "Epoch 86/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2666 - val_loss: 0.2758\n",
      "Epoch 87/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2944 - val_loss: 0.2715\n",
      "Epoch 88/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2601 - val_loss: 0.2731\n",
      "Epoch 89/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2712 - val_loss: 0.2747\n",
      "Epoch 90/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2494 - val_loss: 0.2641\n",
      "Epoch 91/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2788 - val_loss: 0.2902\n",
      "Epoch 92/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2435 - val_loss: 0.2766\n",
      "Epoch 93/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2291 - val_loss: 0.2700\n",
      "Epoch 94/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2421 - val_loss: 0.2706\n",
      "Epoch 95/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2621 - val_loss: 0.2540\n",
      "Epoch 96/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2733 - val_loss: 0.2623\n",
      "Epoch 97/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2628 - val_loss: 0.2726\n",
      "Epoch 98/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2654 - val_loss: 0.2592\n",
      "Epoch 99/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2514 - val_loss: 0.2510\n",
      "Epoch 100/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2187 - val_loss: 0.2538\n",
      "Epoch 101/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2348 - val_loss: 0.2502\n",
      "Epoch 102/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2268 - val_loss: 0.2636\n",
      "Epoch 103/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2421 - val_loss: 0.2788\n",
      "Epoch 104/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2512 - val_loss: 0.2544\n",
      "Epoch 105/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2662 - val_loss: 0.2664\n",
      "Epoch 106/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2442 - val_loss: 0.2792\n",
      "Epoch 107/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2553 - val_loss: 0.2689\n",
      "Epoch 108/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2292 - val_loss: 0.2719\n",
      "Epoch 109/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2189 - val_loss: 0.2635\n",
      "Epoch 110/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2373 - val_loss: 0.2740\n",
      "Epoch 111/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2349 - val_loss: 0.2623\n",
      "Epoch 112/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2130 - val_loss: 0.2667\n",
      "Epoch 113/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2318 - val_loss: 0.2755\n",
      "Epoch 114/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2375 - val_loss: 0.2628\n",
      "Epoch 115/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2351 - val_loss: 0.2613\n",
      "Epoch 116/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2209 - val_loss: 0.2700\n",
      "Epoch 117/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2437 - val_loss: 0.2696\n",
      "Epoch 118/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2392 - val_loss: 0.2664\n",
      "Epoch 119/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2417 - val_loss: 0.2613\n",
      "Epoch 120/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2305 - val_loss: 0.2696\n",
      "Epoch 121/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2352 - val_loss: 0.2587\n",
      "Epoch 122/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2347 - val_loss: 0.2603\n",
      "Epoch 123/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2379 - val_loss: 0.2687\n",
      "Epoch 124/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2330 - val_loss: 0.2641\n",
      "Epoch 125/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2364 - val_loss: 0.2621\n",
      "Epoch 126/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2339 - val_loss: 0.2708\n",
      "Epoch 127/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2276 - val_loss: 0.2689\n",
      "Epoch 128/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2225 - val_loss: 0.2632\n",
      "Epoch 129/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2212 - val_loss: 0.2707\n",
      "Epoch 130/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2256 - val_loss: 0.2588\n",
      "Epoch 131/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2361 - val_loss: 0.2582\n",
      "Epoch 132/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2328 - val_loss: 0.2607\n",
      "Epoch 133/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2274 - val_loss: 0.2663\n",
      "Epoch 134/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2414 - val_loss: 0.2685\n",
      "Epoch 135/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2249 - val_loss: 0.2663\n",
      "Epoch 136/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2146 - val_loss: 0.2579\n",
      "Epoch 137/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2507 - val_loss: 0.2647\n",
      "Epoch 138/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2160 - val_loss: 0.2644\n",
      "Epoch 139/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2389 - val_loss: 0.2558\n",
      "Epoch 140/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2377 - val_loss: 0.2610\n",
      "Epoch 141/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2312 - val_loss: 0.2554\n",
      "Epoch 142/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2502 - val_loss: 0.2590\n",
      "Epoch 143/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2331 - val_loss: 0.2603\n",
      "Epoch 144/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2499 - val_loss: 0.2541\n",
      "Epoch 145/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2791 - val_loss: 0.2622\n",
      "Epoch 146/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2364 - val_loss: 0.2670\n",
      "Epoch 147/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2152 - val_loss: 0.2541\n",
      "Epoch 148/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2139 - val_loss: 0.2670\n",
      "Epoch 149/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2051 - val_loss: 0.2562\n",
      "Epoch 150/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2592 - val_loss: 0.2555\n",
      "Epoch 151/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2415 - val_loss: 0.2570\n",
      "Epoch 152/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2341 - val_loss: 0.2758\n",
      "Epoch 153/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2335 - val_loss: 0.2721\n",
      "Epoch 154/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2404 - val_loss: 0.2594\n",
      "Epoch 155/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2269 - val_loss: 0.2698\n",
      "Epoch 156/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2409 - val_loss: 0.2640\n",
      "Epoch 157/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2483 - val_loss: 0.2692\n",
      "Epoch 158/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2471 - val_loss: 0.2582\n",
      "Epoch 159/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1869 - val_loss: 0.2475\n",
      "Epoch 160/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2266 - val_loss: 0.2701\n",
      "Epoch 161/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2005 - val_loss: 0.2698\n",
      "Epoch 162/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2391 - val_loss: 0.2628\n",
      "Epoch 163/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2252 - val_loss: 0.2645\n",
      "Epoch 164/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2340 - val_loss: 0.2521\n",
      "Epoch 165/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2279 - val_loss: 0.2542\n",
      "Epoch 166/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2263 - val_loss: 0.2501\n",
      "Epoch 167/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2047 - val_loss: 0.2561\n",
      "Epoch 168/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2190 - val_loss: 0.2574\n",
      "Epoch 169/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2028 - val_loss: 0.2568\n",
      "Epoch 170/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2061 - val_loss: 0.2587\n",
      "Epoch 171/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2064 - val_loss: 0.2521\n",
      "Epoch 172/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2286 - val_loss: 0.2616\n",
      "Epoch 173/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2177 - val_loss: 0.2594\n",
      "Epoch 174/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2177 - val_loss: 0.2606\n",
      "Epoch 175/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1969 - val_loss: 0.2546\n",
      "Epoch 176/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2046 - val_loss: 0.2516\n",
      "Epoch 177/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2093 - val_loss: 0.2509\n",
      "Epoch 178/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2140 - val_loss: 0.2546\n",
      "Epoch 179/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2371 - val_loss: 0.2525\n",
      "Epoch 180/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2163 - val_loss: 0.2523\n",
      "Epoch 181/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2315 - val_loss: 0.2574\n",
      "Epoch 182/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2140 - val_loss: 0.2539\n",
      "Epoch 183/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2022 - val_loss: 0.2499\n",
      "Epoch 184/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2184 - val_loss: 0.2543\n",
      "Epoch 185/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2054 - val_loss: 0.2570\n",
      "Epoch 186/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2480 - val_loss: 0.2660\n",
      "Epoch 187/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2066 - val_loss: 0.2578\n",
      "Epoch 188/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2290 - val_loss: 0.2591\n",
      "Epoch 189/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1828 - val_loss: 0.2606\n",
      "Epoch 190/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2186 - val_loss: 0.2589\n",
      "Epoch 191/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2054 - val_loss: 0.2561\n",
      "Epoch 192/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2070 - val_loss: 0.2660\n",
      "Epoch 193/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2400 - val_loss: 0.2616\n",
      "Epoch 194/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2185 - val_loss: 0.2495\n",
      "Epoch 195/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2222 - val_loss: 0.2510\n",
      "Epoch 196/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2367 - val_loss: 0.2678\n",
      "Epoch 197/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2256 - val_loss: 0.2634\n",
      "Epoch 198/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2097 - val_loss: 0.2704\n",
      "Epoch 199/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2193 - val_loss: 0.2486\n",
      "Epoch 200/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2117 - val_loss: 0.2559\n",
      "Epoch 201/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2124 - val_loss: 0.2523\n",
      "Epoch 202/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1964 - val_loss: 0.2588\n",
      "Epoch 203/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2121 - val_loss: 0.2506\n",
      "Epoch 204/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2161 - val_loss: 0.2560\n",
      "Epoch 205/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2068 - val_loss: 0.2443\n",
      "Epoch 206/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2189 - val_loss: 0.2546\n",
      "Epoch 207/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2061 - val_loss: 0.2626\n",
      "Epoch 208/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2234 - val_loss: 0.2626\n",
      "Epoch 209/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2237 - val_loss: 0.2542\n",
      "Epoch 210/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2079 - val_loss: 0.2495\n",
      "Epoch 211/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2108 - val_loss: 0.2600\n",
      "Epoch 212/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2114 - val_loss: 0.2574\n",
      "Epoch 213/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2201 - val_loss: 0.2546\n",
      "Epoch 214/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1872 - val_loss: 0.2546\n",
      "Epoch 215/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2032 - val_loss: 0.2502\n",
      "Epoch 216/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2152 - val_loss: 0.2440\n",
      "Epoch 217/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1972 - val_loss: 0.2516\n",
      "Epoch 218/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1980 - val_loss: 0.2521\n",
      "Epoch 219/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1978 - val_loss: 0.2512\n",
      "Epoch 220/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2128 - val_loss: 0.2571\n",
      "Epoch 221/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2000 - val_loss: 0.2562\n",
      "Epoch 222/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2046 - val_loss: 0.2604\n",
      "Epoch 223/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2049 - val_loss: 0.2718\n",
      "Epoch 224/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2177 - val_loss: 0.2607\n",
      "Epoch 225/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2108 - val_loss: 0.2623\n",
      "Epoch 226/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2135 - val_loss: 0.2521\n",
      "Epoch 227/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1917 - val_loss: 0.2500\n",
      "Epoch 228/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2253 - val_loss: 0.2546\n",
      "Epoch 229/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1899 - val_loss: 0.2579\n",
      "Epoch 230/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2262 - val_loss: 0.2579\n",
      "Epoch 231/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2166 - val_loss: 0.2585\n",
      "Epoch 232/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1931 - val_loss: 0.2553\n",
      "Epoch 233/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2054 - val_loss: 0.2555\n",
      "Epoch 234/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2034 - val_loss: 0.2537\n",
      "Epoch 235/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2242 - val_loss: 0.2515\n",
      "Epoch 236/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1971 - val_loss: 0.2622\n",
      "Epoch 237/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2147 - val_loss: 0.2553\n",
      "Epoch 238/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1949 - val_loss: 0.2507\n",
      "Epoch 239/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2011 - val_loss: 0.2530\n",
      "Epoch 240/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1918 - val_loss: 0.2573\n",
      "Epoch 241/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1931 - val_loss: 0.2518\n",
      "Epoch 242/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2138 - val_loss: 0.2551\n",
      "Epoch 243/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2020 - val_loss: 0.2466\n",
      "Epoch 244/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1937 - val_loss: 0.2546\n",
      "Epoch 245/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1865 - val_loss: 0.2670\n",
      "Epoch 246/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2185 - val_loss: 0.2563\n",
      "Epoch 247/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1997 - val_loss: 0.2587\n",
      "Epoch 248/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2236 - val_loss: 0.2559\n",
      "Epoch 249/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1936 - val_loss: 0.2556\n",
      "Epoch 250/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1969 - val_loss: 0.2540\n",
      "Epoch 251/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2275 - val_loss: 0.2598\n",
      "Epoch 252/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2161 - val_loss: 0.2676\n",
      "Epoch 253/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1852 - val_loss: 0.2597\n",
      "Epoch 254/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1973 - val_loss: 0.2607\n",
      "Epoch 255/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2172 - val_loss: 0.2600\n",
      "Epoch 256/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1853 - val_loss: 0.2627\n",
      "Epoch 257/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1944 - val_loss: 0.2540\n",
      "Epoch 258/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1986 - val_loss: 0.2553\n",
      "Epoch 259/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1996 - val_loss: 0.2569\n",
      "Epoch 260/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2100 - val_loss: 0.2552\n",
      "Epoch 261/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2217 - val_loss: 0.2629\n",
      "Epoch 262/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2075 - val_loss: 0.2563\n",
      "Epoch 263/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2208 - val_loss: 0.2595\n",
      "Epoch 264/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1966 - val_loss: 0.2545\n",
      "Epoch 265/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1958 - val_loss: 0.2619\n",
      "Epoch 266/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2070 - val_loss: 0.2559\n",
      "Epoch 267/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2120 - val_loss: 0.2573\n",
      "Epoch 268/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2057 - val_loss: 0.2599\n",
      "Epoch 269/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1906 - val_loss: 0.2669\n",
      "Epoch 270/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1992 - val_loss: 0.2661\n",
      "Epoch 271/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2010 - val_loss: 0.2547\n",
      "Epoch 272/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2020 - val_loss: 0.2505\n",
      "Epoch 273/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1978 - val_loss: 0.2564\n",
      "Epoch 274/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2105 - val_loss: 0.2569\n",
      "Epoch 275/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1963 - val_loss: 0.2535\n",
      "Epoch 276/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1836 - val_loss: 0.2558\n",
      "Epoch 277/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2032 - val_loss: 0.2501\n",
      "Epoch 278/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1887 - val_loss: 0.2555\n",
      "Epoch 279/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1917 - val_loss: 0.2522\n",
      "Epoch 280/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1951 - val_loss: 0.2566\n",
      "Epoch 281/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1902 - val_loss: 0.2617\n",
      "Epoch 282/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1922 - val_loss: 0.2542\n",
      "Epoch 283/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2088 - val_loss: 0.2521\n",
      "Epoch 284/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1879 - val_loss: 0.2550\n",
      "Epoch 285/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2069 - val_loss: 0.2545\n",
      "Epoch 286/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1890 - val_loss: 0.2670\n",
      "Epoch 287/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2007 - val_loss: 0.2583\n",
      "Epoch 288/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1865 - val_loss: 0.2534\n",
      "Epoch 289/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2138 - val_loss: 0.2483\n",
      "Epoch 290/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2226 - val_loss: 0.2521\n",
      "Epoch 291/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1915 - val_loss: 0.2522\n",
      "Epoch 292/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1842 - val_loss: 0.2569\n",
      "Epoch 293/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1725 - val_loss: 0.2573\n",
      "Epoch 294/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2094 - val_loss: 0.2539\n",
      "Epoch 295/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2060 - val_loss: 0.2518\n",
      "Epoch 296/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1853 - val_loss: 0.2530\n",
      "Epoch 297/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2154 - val_loss: 0.2534\n",
      "Epoch 298/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2098 - val_loss: 0.2500\n",
      "Epoch 299/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1917 - val_loss: 0.2586\n",
      "Epoch 300/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2258 - val_loss: 0.2463\n",
      "Epoch 301/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2074 - val_loss: 0.2583\n",
      "Epoch 302/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1962 - val_loss: 0.2561\n",
      "Epoch 303/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2007 - val_loss: 0.2542\n",
      "Epoch 304/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1737 - val_loss: 0.2587\n",
      "Epoch 305/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2015 - val_loss: 0.2508\n",
      "Epoch 306/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1924 - val_loss: 0.2480\n",
      "Epoch 307/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1984 - val_loss: 0.2475\n",
      "Epoch 308/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1836 - val_loss: 0.2523\n",
      "Epoch 309/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1859 - val_loss: 0.2502\n",
      "Epoch 310/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1928 - val_loss: 0.2551\n",
      "Epoch 311/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2123 - val_loss: 0.2489\n",
      "Epoch 312/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1913 - val_loss: 0.2522\n",
      "Epoch 313/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1898 - val_loss: 0.2625\n",
      "Epoch 314/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1886 - val_loss: 0.2504\n",
      "Epoch 315/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1900 - val_loss: 0.2551\n",
      "Epoch 316/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1996 - val_loss: 0.2562\n",
      "Epoch 317/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1903 - val_loss: 0.2601\n",
      "Epoch 318/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1853 - val_loss: 0.2518\n",
      "Epoch 319/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2046 - val_loss: 0.2555\n",
      "Epoch 320/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1878 - val_loss: 0.2613\n",
      "Epoch 321/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1977 - val_loss: 0.2543\n",
      "Epoch 322/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2060 - val_loss: 0.2553\n",
      "Epoch 323/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1906 - val_loss: 0.2561\n",
      "Epoch 324/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1982 - val_loss: 0.2495\n",
      "Epoch 325/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1893 - val_loss: 0.2541\n",
      "Epoch 326/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2156 - val_loss: 0.2523\n",
      "Epoch 327/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1900 - val_loss: 0.2573\n",
      "Epoch 328/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2055 - val_loss: 0.2553\n",
      "Epoch 329/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1936 - val_loss: 0.2621\n",
      "Epoch 330/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1784 - val_loss: 0.2606\n",
      "Epoch 331/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1941 - val_loss: 0.2527\n",
      "Epoch 332/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1870 - val_loss: 0.2518\n",
      "Epoch 333/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1845 - val_loss: 0.2650\n",
      "Epoch 334/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1952 - val_loss: 0.2611\n",
      "Epoch 335/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2035 - val_loss: 0.2733\n",
      "Epoch 336/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2069 - val_loss: 0.2563\n",
      "Epoch 337/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2037 - val_loss: 0.2672\n",
      "Epoch 338/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1859 - val_loss: 0.2620\n",
      "Epoch 339/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1947 - val_loss: 0.2541\n",
      "Epoch 340/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1835 - val_loss: 0.2568\n",
      "Epoch 341/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1964 - val_loss: 0.2617\n",
      "Epoch 342/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2144 - val_loss: 0.2609\n",
      "Epoch 343/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1943 - val_loss: 0.2588\n",
      "Epoch 344/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1691 - val_loss: 0.2606\n",
      "Epoch 345/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1961 - val_loss: 0.2636\n",
      "Epoch 346/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1857 - val_loss: 0.2543\n",
      "Epoch 347/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1880 - val_loss: 0.2591\n",
      "Epoch 348/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1899 - val_loss: 0.2566\n",
      "Epoch 349/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1839 - val_loss: 0.2598\n",
      "Epoch 350/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1885 - val_loss: 0.2554\n",
      "Epoch 351/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1879 - val_loss: 0.2585\n",
      "Epoch 352/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2313 - val_loss: 0.2576\n",
      "Epoch 353/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2021 - val_loss: 0.2583\n",
      "Epoch 354/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2086 - val_loss: 0.2596\n",
      "Epoch 355/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2029 - val_loss: 0.2615\n",
      "Epoch 356/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2126 - val_loss: 0.2668\n",
      "Epoch 357/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1879 - val_loss: 0.2670\n",
      "Epoch 358/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2003 - val_loss: 0.2594\n",
      "Epoch 359/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1909 - val_loss: 0.2568\n",
      "Epoch 360/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1904 - val_loss: 0.2569\n",
      "Epoch 361/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2041 - val_loss: 0.2539\n",
      "Epoch 362/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1608 - val_loss: 0.2536\n",
      "Epoch 363/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1807 - val_loss: 0.2541\n",
      "Epoch 364/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1724 - val_loss: 0.2561\n",
      "Epoch 365/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1927 - val_loss: 0.2691\n",
      "Epoch 366/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1943 - val_loss: 0.2653\n",
      "Epoch 367/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1856 - val_loss: 0.2620\n",
      "Epoch 368/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2064 - val_loss: 0.2603\n",
      "Epoch 369/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1778 - val_loss: 0.2620\n",
      "Epoch 370/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1818 - val_loss: 0.2733\n",
      "Epoch 371/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1917 - val_loss: 0.2632\n",
      "Epoch 372/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2040 - val_loss: 0.2549\n",
      "Epoch 373/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1759 - val_loss: 0.2593\n",
      "Epoch 374/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1994 - val_loss: 0.2540\n",
      "Epoch 375/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1985 - val_loss: 0.2614\n",
      "Epoch 376/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1813 - val_loss: 0.2641\n",
      "Epoch 377/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1888 - val_loss: 0.2633\n",
      "Epoch 378/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2017 - val_loss: 0.2626\n",
      "Epoch 379/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2040 - val_loss: 0.2644\n",
      "Epoch 380/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1902 - val_loss: 0.2635\n",
      "Epoch 381/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1735 - val_loss: 0.2590\n",
      "Epoch 382/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1817 - val_loss: 0.2551\n",
      "Epoch 383/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1905 - val_loss: 0.2536\n",
      "Epoch 384/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1896 - val_loss: 0.2563\n",
      "Epoch 385/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2098 - val_loss: 0.2605\n",
      "Epoch 386/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1768 - val_loss: 0.2540\n",
      "Epoch 387/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1829 - val_loss: 0.2565\n",
      "Epoch 388/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1883 - val_loss: 0.2668\n",
      "Epoch 389/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1900 - val_loss: 0.2558\n",
      "Epoch 390/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1681 - val_loss: 0.2604\n",
      "Epoch 391/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1772 - val_loss: 0.2607\n",
      "Epoch 392/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1815 - val_loss: 0.2603\n",
      "Epoch 393/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1815 - val_loss: 0.2535\n",
      "Epoch 394/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1916 - val_loss: 0.2578\n",
      "Epoch 395/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1773 - val_loss: 0.2603\n",
      "Epoch 396/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1723 - val_loss: 0.2580\n",
      "Epoch 397/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1799 - val_loss: 0.2588\n",
      "Epoch 398/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1889 - val_loss: 0.2572\n",
      "Epoch 399/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1832 - val_loss: 0.2576\n",
      "Epoch 400/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1902 - val_loss: 0.2597\n",
      "Epoch 401/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1914 - val_loss: 0.2596\n",
      "Epoch 402/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1728 - val_loss: 0.2589\n",
      "Epoch 403/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1793 - val_loss: 0.2594\n",
      "Epoch 404/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1715 - val_loss: 0.2535\n",
      "Epoch 405/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1889 - val_loss: 0.2542\n",
      "Epoch 406/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1727 - val_loss: 0.2558\n",
      "Epoch 407/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1740 - val_loss: 0.2555\n",
      "Epoch 408/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1882 - val_loss: 0.2534\n",
      "Epoch 409/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1702 - val_loss: 0.2640\n",
      "Epoch 410/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1938 - val_loss: 0.2548\n",
      "Epoch 411/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2164 - val_loss: 0.2628\n",
      "Epoch 412/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1864 - val_loss: 0.2569\n",
      "Epoch 413/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1915 - val_loss: 0.2603\n",
      "Epoch 414/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1959 - val_loss: 0.2557\n",
      "Epoch 415/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1887 - val_loss: 0.2588\n",
      "Epoch 416/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1552 - val_loss: 0.2560\n",
      "Epoch 417/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1827 - val_loss: 0.2555\n",
      "Epoch 418/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1730 - val_loss: 0.2534\n",
      "Epoch 419/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1652 - val_loss: 0.2541\n",
      "Epoch 420/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1828 - val_loss: 0.2602\n",
      "Epoch 421/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1842 - val_loss: 0.2590\n",
      "Epoch 422/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1833 - val_loss: 0.2521\n",
      "Epoch 423/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1778 - val_loss: 0.2549\n",
      "Epoch 424/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1819 - val_loss: 0.2575\n",
      "Epoch 425/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1743 - val_loss: 0.2566\n",
      "Epoch 426/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1687 - val_loss: 0.2545\n",
      "Epoch 427/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1651 - val_loss: 0.2624\n",
      "Epoch 428/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1852 - val_loss: 0.2565\n",
      "Epoch 429/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1891 - val_loss: 0.2560\n",
      "Epoch 430/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1792 - val_loss: 0.2613\n",
      "Epoch 431/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1896 - val_loss: 0.2620\n",
      "Epoch 432/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1842 - val_loss: 0.2559\n",
      "Epoch 433/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1845 - val_loss: 0.2566\n",
      "Epoch 434/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1987 - val_loss: 0.2580\n",
      "Epoch 435/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1862 - val_loss: 0.2528\n",
      "Epoch 436/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1818 - val_loss: 0.2564\n",
      "Epoch 437/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1967 - val_loss: 0.2494\n",
      "Epoch 438/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1691 - val_loss: 0.2465\n",
      "Epoch 439/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1693 - val_loss: 0.2501\n",
      "Epoch 440/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2015 - val_loss: 0.2517\n",
      "Epoch 441/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1766 - val_loss: 0.2618\n",
      "Epoch 442/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1640 - val_loss: 0.2546\n",
      "Epoch 443/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1855 - val_loss: 0.2556\n",
      "Epoch 444/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1647 - val_loss: 0.2532\n",
      "Epoch 445/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1911 - val_loss: 0.2625\n",
      "Epoch 446/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2032 - val_loss: 0.2530\n",
      "Epoch 447/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1962 - val_loss: 0.2508\n",
      "Epoch 448/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1988 - val_loss: 0.2491\n",
      "Epoch 449/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1710 - val_loss: 0.2435\n",
      "Epoch 450/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1815 - val_loss: 0.2454\n",
      "Epoch 451/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1750 - val_loss: 0.2477\n",
      "Epoch 452/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1901 - val_loss: 0.2500\n",
      "Epoch 453/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1913 - val_loss: 0.2592\n",
      "Epoch 454/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1812 - val_loss: 0.2539\n",
      "Epoch 455/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1806 - val_loss: 0.2501\n",
      "Epoch 456/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1642 - val_loss: 0.2513\n",
      "Epoch 457/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1722 - val_loss: 0.2562\n",
      "Epoch 458/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1834 - val_loss: 0.2534\n",
      "Epoch 459/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1761 - val_loss: 0.2535\n",
      "Epoch 460/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1901 - val_loss: 0.2517\n",
      "Epoch 461/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2046 - val_loss: 0.2528\n",
      "Epoch 462/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1815 - val_loss: 0.2520\n",
      "Epoch 463/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1742 - val_loss: 0.2572\n",
      "Epoch 464/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1905 - val_loss: 0.2517\n",
      "Epoch 465/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1645 - val_loss: 0.2522\n",
      "Epoch 466/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1822 - val_loss: 0.2551\n",
      "Epoch 467/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1848 - val_loss: 0.2540\n",
      "Epoch 468/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1744 - val_loss: 0.2510\n",
      "Epoch 469/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1831 - val_loss: 0.2513\n",
      "Epoch 470/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1978 - val_loss: 0.2504\n",
      "Epoch 471/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1691 - val_loss: 0.2531\n",
      "Epoch 472/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1965 - val_loss: 0.2528\n",
      "Epoch 473/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2069 - val_loss: 0.2565\n",
      "Epoch 474/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1731 - val_loss: 0.2604\n",
      "Epoch 475/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1909 - val_loss: 0.2628\n",
      "Epoch 476/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1878 - val_loss: 0.2678\n",
      "Epoch 477/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1781 - val_loss: 0.2679\n",
      "Epoch 478/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2058 - val_loss: 0.2696\n",
      "Epoch 479/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1938 - val_loss: 0.2653\n",
      "Epoch 480/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1914 - val_loss: 0.2534\n",
      "Epoch 481/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1744 - val_loss: 0.2536\n",
      "Epoch 482/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1890 - val_loss: 0.2516\n",
      "Epoch 483/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1765 - val_loss: 0.2536\n",
      "Epoch 484/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1933 - val_loss: 0.2526\n",
      "Epoch 485/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1626 - val_loss: 0.2526\n",
      "Epoch 486/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1772 - val_loss: 0.2507\n",
      "Epoch 487/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1881 - val_loss: 0.2576\n",
      "Epoch 488/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1760 - val_loss: 0.2580\n",
      "Epoch 489/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1859 - val_loss: 0.2568\n",
      "Epoch 490/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1634 - val_loss: 0.2574\n",
      "Epoch 491/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1806 - val_loss: 0.2593\n",
      "Epoch 492/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1677 - val_loss: 0.2532\n",
      "Epoch 493/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1694 - val_loss: 0.2569\n",
      "Epoch 494/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1925 - val_loss: 0.2568\n",
      "Epoch 495/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1662 - val_loss: 0.2640\n",
      "Epoch 496/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1894 - val_loss: 0.2584\n",
      "Epoch 497/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1834 - val_loss: 0.2578\n",
      "Epoch 498/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1751 - val_loss: 0.2548\n",
      "Epoch 499/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1823 - val_loss: 0.2559\n",
      "Epoch 500/500\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1878 - val_loss: 0.2628\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7ed78cc8a170>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# learning_rate = 0.001\n",
    "# from keras import optimizers, callbacks\n",
    "\n",
    "# model = models.Sequential([\n",
    "#     layers.Dropout(0.5),  # Regularization\n",
    "#     layers.Dense(200, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
    "#     layers.Dropout(0.5),\n",
    "#     layers.Dense(400, activation='relu'),  # Increased complexity\n",
    "#     layers.Dropout(0.5),\n",
    "#     layers.Dense(400, activation='relu'),  # Increased complexity\n",
    "#     layers.Dense(200, activation='relu'),  # Increased complexity\n",
    "#     layers.Dropout(0.5),\n",
    "#     layers.Dense(200, activation='relu'),\n",
    "#     layers.Dense(100, activation='relu'),\n",
    "#     layers.Dropout(0.5),\n",
    "#     layers.Dense(50, activation='relu'),\n",
    "#     layers.Dense(1)  # Output layer\n",
    "# ])\n",
    "\n",
    "# # Compile the model with a lower learning rate\n",
    "# model.compile(optimizer=optimizers.Adam(learning_rate=learning_rate), loss='mean_squared_error')\n",
    "\n",
    "# # Train the model with more epochs\n",
    "# model.fit(X_train_scaled, y_train, epochs=500, validation_split=0.3 , batch_size=15, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "save_model(model, 'my_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2696 \n",
      "Test Loss: 0.2840840816497803\n"
     ]
    }
   ],
   "source": [
    "\n",
    "loss = model.evaluate(X_test_scaled, y_test)\n",
    "print(\"Test Loss:\", loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.array(predictions).reshape(-1)  # Reshape predictions to be 1-dimensional\n",
    "y_test = np.array(y_test).reshape(-1)            # Reshape y_test to be 1-dimensional\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predictions</th>\n",
       "      <th>Targets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.474678</td>\n",
       "      <td>1.481486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.550715</td>\n",
       "      <td>0.744727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.287363</td>\n",
       "      <td>1.301030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.525407</td>\n",
       "      <td>0.301030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.450102</td>\n",
       "      <td>0.221849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376</th>\n",
       "      <td>1.194717</td>\n",
       "      <td>1.301030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377</th>\n",
       "      <td>1.419920</td>\n",
       "      <td>2.522879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378</th>\n",
       "      <td>1.427891</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>1.424452</td>\n",
       "      <td>1.187087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>1.377741</td>\n",
       "      <td>0.408935</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>381 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Predictions   Targets\n",
       "0       0.474678  1.481486\n",
       "1       0.550715  0.744727\n",
       "2       1.287363  1.301030\n",
       "3       0.525407  0.301030\n",
       "4       0.450102  0.221849\n",
       "..           ...       ...\n",
       "376     1.194717  1.301030\n",
       "377     1.419920  2.522879\n",
       "378     1.427891  2.000000\n",
       "379     1.424452  1.187087\n",
       "380     1.377741  0.408935\n",
       "\n",
       "[381 rows x 2 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame({'Predictions': predictions, 'Targets': y_test})\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "mae = mean_absolute_error(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "r2 = r2_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.49008573978748216 0.26042744887717706\n"
     ]
    }
   ],
   "source": [
    "print(r2, mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdkit.Chem.rdchem.Mol"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a =structure.smile_to_image(\"CC\")\n",
    "type(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
